{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word2vecmodel = gensim.models.KeyedVectors.load_word2vec_format(path.expanduser('~/work/jupyter_notebooks/notebooks/GoogleNews-vectors-negative300.bin'), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecmodel.save(path.expanduser('~/work/data/word2vec.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecmodel = gensim.models.KeyedVectors.load(path.expanduser('~/work/data/word2vec.bin'), mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50956"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://www.ted.com/talks/craig_costello_cryptographers_quantum_computers_and_the_war_for_information'\n",
    "title = url.split('/')[-1]\n",
    "metaURL = 'https://api.ted.com/v1/talks/{}.json?api-key=uzdyad5pnc2mv2dd8r8vd65c'\n",
    "r = requests.get(url = metaURL.format(title))\n",
    "data = r.json()\n",
    "id = data['talk']['id']\n",
    "# [tag['tag'] for tag in data['talk']['tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import re\n",
    "\n",
    "with open(path.expanduser('~/work/data/data.pkl'), \"rb\") as input_file:\n",
    "    data = pickle.load(input_file)\n",
    "    \n",
    "tags = [re.sub('\\'','',taglist)[1:-1].split(', ') for taglist in data['tags'].values]\n",
    "file = open(path.expanduser('~/work/data/tags.txt'), \"w\")\n",
    "for talktags in tags:\n",
    "    file.write(' '.join(talktags) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def retrieve_prepare_subtitles(url):\n",
    "    \n",
    "    metaDataURL = 'https://api.ted.com/v1/talks/{}.json?api-key=uzdyad5pnc2mv2dd8r8vd65c'\n",
    "    subURL = 'https://api.ted.com/v1/talks/{}/subtitles.json?api-key=uzdyad5pnc2mv2dd8r8vd65c'\n",
    "    r = requests.get(url = subURL.format(id))\n",
    "    data = r.json() \n",
    "    transcript = []\n",
    "    for dict in np.arange(len(data)-1):\n",
    "        text = data[str(dict)]['caption']['content']\n",
    "        text = re.sub(r'\\((.*?)\\)', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n",
    "        text = [w for w in text if w not in nltk.corpus.stopwords.words('english')]\n",
    "        text = [nltk.corpus.wordnet.morphy(w) if nltk.corpus.wordnet.morphy(w) else w for w in text]\n",
    "        transcript.extend(text)\n",
    "    return ' '.join(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file = open(path.expanduser('~/work/test/lftm/doc.txt'), \"w\")\n",
    "file.write(retrieve_prepare_subtitles(176)) \n",
    "file.close()\n",
    "\n",
    "model = 'LFLDAinf'\n",
    "paras = 'TEDLFLDA.paras'\n",
    "corpus = 'doc.txt'\n",
    "initer = '500'\n",
    "niter = '50'\n",
    "topn = '20'\n",
    "name = 'TEDLFLDAinf'\n",
    "sstep = '0'\n",
    "\n",
    "os.chdir(os.path.expanduser('~/work/test/lftm/'))\n",
    "os.system('java -jar jar/LFTM.jar -model {} -paras {} -corpus {} -initers {} -niters {} -twords {} -name {} -sstep {}'.format(\n",
    "    model,\n",
    "    paras,\n",
    "    corpus,\n",
    "    initer,\n",
    "    niter,\n",
    "    topn,\n",
    "    name,\n",
    "    sstep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(path.expanduser('~/work/test/lftm/TEDLFLDAinf.theta'), \"r\")\n",
    "topics = file.readline()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(datapath):\n",
    "    text = []\n",
    "    with open(datapath,\"r+\") as f:\n",
    "        while True:\n",
    "            vec = f.readline()\n",
    "            text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "with open(path.expanduser('~/work/data/data.pkl'), \"rb\") as input_file:\n",
    "    data = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "metaDataURL = 'https://api.ted.com/v1/talks/{}.json?api-key=uzdyad5pnc2mv2dd8r8vd65c'\n",
    "subURL = 'https://api.ted.com/v1/talks/{}/subtitles.json?api-key=uzdyad5pnc2mv2dd8r8vd65c'\n",
    "_id=1\n",
    "\n",
    "talk_meta = pd.DataFrame()\n",
    "talk_sub = pd.DataFrame()\n",
    "talk = {}\n",
    "sub = {}\n",
    "while True:\n",
    "    response_meta = requests.get(metaDataURL.format(_id))\n",
    "    response_meta = response_meta.json()\n",
    "    if 'error' in response_meta:\n",
    "        print(\"Id\",_id, \"not found.\")\n",
    "        _id += 1\n",
    "        continue\n",
    "    response_sub = requests.get(subURL.format(_id))\n",
    "    response_sub = response_sub.json()\n",
    "    if 'error' in response_sub:\n",
    "        print(\"Sub for\",_id, \"not found.\")\n",
    "        _id += 1\n",
    "        continue\n",
    "    talk['talk_id'] = _id\n",
    "    talk['name'] = response_meta['talk']['name']\n",
    "    talk['tags'] = ','.join([tag['tag'] for tag in response_meta['talk']['tags']])\n",
    "    talk_meta = talk_meta.append(talk, ignore_index=True)\n",
    "    \n",
    "    chapter_id = 0\n",
    "    chapter = []\n",
    "    \n",
    "    no_caption = len(response_sub)\n",
    "    chapter.append(response_sub['0']['caption']['content'])\n",
    "    \n",
    "    for dict in np.arange(2,no_caption - 2):\n",
    "        if response_sub[str(dict)]['caption']['startOfParagraph']:\n",
    "            sub['talk_id'] = _id\n",
    "            sub['id'] = chapter_id\n",
    "            sub['transcript'] = ' '.join(chapter)\n",
    "            talk_sub = talk_sub.append(sub, ignore_index=True)\n",
    "            chapter_id += 1\n",
    "            chapter = []\n",
    "            chapter.append(response_sub[str(dict)]['caption']['content'])\n",
    "        else:\n",
    "            chapter.append(response_sub[str(dict)]['caption']['content'])\n",
    "            \n",
    "    if response_sub[str(no_caption - 2)]['caption']['startOfParagraph']:\n",
    "        sub['talk_id'] = _id\n",
    "        sub['id'] = chapter_id\n",
    "        sub['transcript'] = ' '.join(chapter)\n",
    "        talk_sub = talk_sub.append(sub, ignore_index=True)\n",
    "        chapter_id += 1\n",
    "        chapter = []\n",
    "        chapter.append(response_sub[str(no_caption - 2)]['caption']['content'])\n",
    "        sub['talk_id'] = _id\n",
    "        sub['id'] = chapter_id\n",
    "        sub['transcript'] = ' '.join(chapter)\n",
    "        talk_sub = talk_sub.append(sub, ignore_index=True)\n",
    "    else:\n",
    "        sub['talk_id'] = _id\n",
    "        sub['id'] = chapter_id\n",
    "        chapter.append(response_sub[str(no_caption - 2)]['caption']['content'])\n",
    "        sub['transcript'] = ' '.join(chapter)\n",
    "        talk_sub = talk_sub.append(sub, ignore_index=True)\n",
    "                \n",
    "#         text = re.sub(r'\\((.*?)\\)', ' ', text)\n",
    "#         text = re.sub(r'\\d+', '', text)\n",
    "#         text = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n",
    "#         text = [w for w in text if w not in nltk.corpus.stopwords.words('english')]\n",
    "#         text = [nltk.corpus.wordnet.morphy(w) if nltk.corpus.wordnet.morphy(w) else w for w in text]\n",
    "#         transcript.extend(text)\n",
    "    print(\"Loaded talk\",_id)\n",
    "    _id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4.0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5.0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3642.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3671.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680.0</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3686.0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3694.0</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2560 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  transcript\n",
       "talk_id                \n",
       "1.0      65          65\n",
       "2.0      39          39\n",
       "3.0      22          22\n",
       "4.0      28          28\n",
       "5.0      28          28\n",
       "...      ..         ...\n",
       "3642.0   12          12\n",
       "3671.0    8           8\n",
       "3680.0   35          35\n",
       "3686.0   16          16\n",
       "3694.0   56          56\n",
       "\n",
       "[2560 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_sub.groupby('talk_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0,\n",
       "        \"Thank you so much, Chris. to come to this stage twice;\\nI'm extremely grateful. I have been blown away by this conference, and I want to thank all of you\\nfor the many nice comments about what I had to say the other night. And I say that sincerely, partly because (Mock sob) I need that.\"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_sub[(talk_sub.talk_id ==1) & (talk_sub.id == 0)].sort_values('id').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>talk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>alternative energy,cars,climate change,culture...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Amy Smith: Simple designs to save a life</td>\n",
       "      <td>MacArthur grant,alternative energy,design,engi...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ashraf Ghani: How to rebuild a broken state</td>\n",
       "      <td>business,corruption,culture,economics,entrepre...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Burt Rutan: The real future of space exploration</td>\n",
       "      <td>NASA,aircraft,business,design,engineering,entr...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Chris Bangle: Great cars are great art</td>\n",
       "      <td>art,business,cars,design,industrial design,inv...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2555</td>\n",
       "      <td>Niti Bhan: The hidden opportunities of the inf...</td>\n",
       "      <td>Africa,business,creativity,culture,economics,i...</td>\n",
       "      <td>3642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2556</td>\n",
       "      <td>Matilda Ho: The future of good food in China</td>\n",
       "      <td>TED Fellows,agriculture,china,entrepreneur,far...</td>\n",
       "      <td>3671.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2557</td>\n",
       "      <td>Gretchen Carlson: How we can end sexual harass...</td>\n",
       "      <td>activism,journalism,law,leadership,personal gr...</td>\n",
       "      <td>3680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2558</td>\n",
       "      <td>Dan Gartenberg: The brain benefits of deep sle...</td>\n",
       "      <td>Human body,TED Residency,brain,health,memory,s...</td>\n",
       "      <td>3686.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2559</td>\n",
       "      <td>Keller Rinaudo: How we're using drones to deli...</td>\n",
       "      <td>Africa,disease,drones,entrepreneur,health,heal...</td>\n",
       "      <td>3694.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2560 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name  \\\n",
       "0                  Al Gore: Averting the climate crisis   \n",
       "1              Amy Smith: Simple designs to save a life   \n",
       "2           Ashraf Ghani: How to rebuild a broken state   \n",
       "3      Burt Rutan: The real future of space exploration   \n",
       "4                Chris Bangle: Great cars are great art   \n",
       "...                                                 ...   \n",
       "2555  Niti Bhan: The hidden opportunities of the inf...   \n",
       "2556       Matilda Ho: The future of good food in China   \n",
       "2557  Gretchen Carlson: How we can end sexual harass...   \n",
       "2558  Dan Gartenberg: The brain benefits of deep sle...   \n",
       "2559  Keller Rinaudo: How we're using drones to deli...   \n",
       "\n",
       "                                                   tags  talk_id  \n",
       "0     alternative energy,cars,climate change,culture...      1.0  \n",
       "1     MacArthur grant,alternative energy,design,engi...      2.0  \n",
       "2     business,corruption,culture,economics,entrepre...      3.0  \n",
       "3     NASA,aircraft,business,design,engineering,entr...      4.0  \n",
       "4     art,business,cars,design,industrial design,inv...      5.0  \n",
       "...                                                 ...      ...  \n",
       "2555  Africa,business,creativity,culture,economics,i...   3642.0  \n",
       "2556  TED Fellows,agriculture,china,entrepreneur,far...   3671.0  \n",
       "2557  activism,journalism,law,leadership,personal gr...   3680.0  \n",
       "2558  Human body,TED Residency,brain,health,memory,s...   3686.0  \n",
       "2559  Africa,disease,drones,entrepreneur,health,heal...   3694.0  \n",
       "\n",
       "[2560 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_data = talk_meta.set_index('talk_id').join(talk_sub.set_index('talk_id'), 'talk_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_data = talk_meta.set_index('talk_id').join(talk_sub.set_index('talk_id'), on = 'talk_id')\n",
    "talk_meta = talk_meta[~talk_meta['talk_id'].isin(talk_data[talk_data['transcript'].isna()].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.expanduser('~/work/data/talk_meta.pkl'), 'wb') as output:\n",
    "    pickle.dump(talk_meta, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(path.expanduser('~/work/data/talk_sub.pkl'), 'wb') as output:\n",
    "    pickle.dump(talk_sub, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pickle\n",
    "\n",
    "with open(path.expanduser('~/work/data/talk_meta.pkl'), \"rb\") as input_file:\n",
    "    talk_meta = pickle.load(input_file)\n",
    "with open(path.expanduser('~/work/data/talk_sub.pkl'), \"rb\") as input_file:\n",
    "    talk_sub = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1263.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1677.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2147.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2273.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010.0</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254.0</td>\n",
       "      <td>220</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2209.0</td>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500.0</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823.0</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2481 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  transcript\n",
       "talk_id                 \n",
       "1263.0     1           1\n",
       "1430.0     1           1\n",
       "1677.0     1           1\n",
       "2147.0     1           1\n",
       "2273.0     2           2\n",
       "...      ...         ...\n",
       "2010.0   215         215\n",
       "254.0    220         220\n",
       "2209.0   230         230\n",
       "500.0    255         255\n",
       "823.0    275         275\n",
       "\n",
       "[2481 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_sub.groupby('talk_id').count().sort_values('transcript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>talk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>alternative energy,cars,climate change,culture...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Amy Smith: Simple designs to save a life</td>\n",
       "      <td>MacArthur grant,alternative energy,design,engi...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ashraf Ghani: How to rebuild a broken state</td>\n",
       "      <td>business,corruption,culture,economics,entrepre...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Burt Rutan: The real future of space exploration</td>\n",
       "      <td>NASA,aircraft,business,design,engineering,entr...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Chris Bangle: Great cars are great art</td>\n",
       "      <td>art,business,cars,design,industrial design,inv...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2573</td>\n",
       "      <td>Anjan Sundaram: Why I risked my life to expose...</td>\n",
       "      <td>Africa,TED Fellows,corruption,journalism,war</td>\n",
       "      <td>3363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2574</td>\n",
       "      <td>Mike Kinney: A pro wrestler's guide to confidence</td>\n",
       "      <td>business,creativity,humor,identity</td>\n",
       "      <td>3364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>Christen Reighter: I don't want children -- st...</td>\n",
       "      <td>TEDx,children,family,identity,parenting,social...</td>\n",
       "      <td>3365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2576</td>\n",
       "      <td>Chris Sheldrick: A precise, three-word address...</td>\n",
       "      <td>communication,cooperation,global development,i...</td>\n",
       "      <td>3366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2577</td>\n",
       "      <td>Margrethe Vestager: The new age of corporate m...</td>\n",
       "      <td>Europe,business,corruption,economics,global is...</td>\n",
       "      <td>3367.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2481 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name  \\\n",
       "0                  Al Gore: Averting the climate crisis   \n",
       "1              Amy Smith: Simple designs to save a life   \n",
       "2           Ashraf Ghani: How to rebuild a broken state   \n",
       "3      Burt Rutan: The real future of space exploration   \n",
       "4                Chris Bangle: Great cars are great art   \n",
       "...                                                 ...   \n",
       "2573  Anjan Sundaram: Why I risked my life to expose...   \n",
       "2574  Mike Kinney: A pro wrestler's guide to confidence   \n",
       "2575  Christen Reighter: I don't want children -- st...   \n",
       "2576  Chris Sheldrick: A precise, three-word address...   \n",
       "2577  Margrethe Vestager: The new age of corporate m...   \n",
       "\n",
       "                                                   tags  talk_id  \n",
       "0     alternative energy,cars,climate change,culture...      1.0  \n",
       "1     MacArthur grant,alternative energy,design,engi...      2.0  \n",
       "2     business,corruption,culture,economics,entrepre...      3.0  \n",
       "3     NASA,aircraft,business,design,engineering,entr...      4.0  \n",
       "4     art,business,cars,design,industrial design,inv...      5.0  \n",
       "...                                                 ...      ...  \n",
       "2573       Africa,TED Fellows,corruption,journalism,war   3363.0  \n",
       "2574                 business,creativity,humor,identity   3364.0  \n",
       "2575  TEDx,children,family,identity,parenting,social...   3365.0  \n",
       "2576  communication,cooperation,global development,i...   3366.0  \n",
       "2577  Europe,business,corruption,economics,global is...   3367.0  \n",
       "\n",
       "[2481 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449906 sha256=28275f2290bf7b36a75865276ea58317c4c97f28a7560fe571a6fce08335cd4b\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "def prepare_text(text):\n",
    "    text = re.sub(r'\\((.*?)\\)', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n",
    "    text = [w for w in text if not((w in ['thank', 'much']) or (w in nltk.corpus.stopwords.words('english')))]\n",
    "    text = [nltk.corpus.wordnet.morphy(w) if nltk.corpus.wordnet.morphy(w) else w for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['chris', 'truly', 'great', 'honor', 'opportunity', 'come', 'stage', 'twice', 'extremely', 'grateful', 'blow', 'away', 'conference', 'want', 'many', 'nice', 'comment', 'say', 'night', 'say', 'sincerely', 'partly', 'need']),\n",
       "       list([]), list(['put', 'position']), list([]),\n",
       "       list(['fly', 'air', 'force', 'two', 'eight', 'years']), list([]),\n",
       "       list(['take', 'shoes', 'boot', 'get', 'airplane']), list([]),\n",
       "       list([]),\n",
       "       list(['tell', 'one', 'quick', 'story', 'illustrate', 'like']),\n",
       "       list([]), list(['true', 'story', 'every', 'bit', 'true']),\n",
       "       list(['soon', 'tipper', 'left', 'white', 'house']), list([]),\n",
       "       list(['driving', 'home', 'nashville', 'little', 'farm', 'mile', 'east', 'nashville', 'driving']),\n",
       "       list([]), list(['know', 'sound', 'like', 'little', 'thing']),\n",
       "       list([]),\n",
       "       list(['look', 'rear', 'view', 'mirror', 'sudden', 'hit', 'motorcade', 'back']),\n",
       "       list([]), list(['hear', 'phantom', 'limb', 'pain']), list([]),\n",
       "       list(['rent', 'ford', 'taurus']), list([]),\n",
       "       list(['dinnertime', 'start', 'looking', 'place', 'eat', 'get', 'exit', 'lebanon', 'tennessee', 'get', 'exit', 'found', 'shoney', 'restaurant', 'low', 'cost', 'family', 'restaurant', 'chain', 'know', 'go', 'sat', 'booth', 'waitress', 'come', 'make', 'big', 'commotion', 'tipper']),\n",
       "       list([]),\n",
       "       list(['take', 'order', 'go', 'couple', 'booth', 'next', 'us', 'lower', 'voice', 'really', 'strain', 'hear', 'saying', 'say', 'yes', 'former', 'vice', 'president', 'al', 'gore', 'wife', 'tipper', 'man', 'say', 'come', 'long', 'way']),\n",
       "       list([]), list([]), list(['kind', 'series', 'epiphany']), list([]),\n",
       "       list(['next', 'day', 'continue', 'totally', 'true', 'story', 'get', 'g', 'v', 'fly', 'africa', 'make', 'speech', 'nigeria', 'city', 'lagos', 'topic', 'energy', 'begin', 'speech', 'telling', 'story', 'happen', 'day', 'nashville', 'tell', 'pretty', 'way', 'share', 'tipper', 'driving', 'shoney', 'low', 'cost', 'family', 'restaurant', 'chain', 'man', 'say', 'laugh', 'give', 'speech', 'go', 'back', 'airport', 'fly', 'back', 'home', 'fell', 'asleep', 'plane', 'middle', 'night', 'land', 'azores', 'island', 'refueling', 'wake', 'open', 'door', 'go', 'get', 'fresh', 'air', 'look', 'man', 'running', 'across', 'runway', 'waving', 'piece', 'paper', 'yelling', 'call', 'washington', 'call', 'washington', 'thought', 'middle', 'night', 'middle', 'atlantic', 'world', 'could', 'wrong', 'washington', 'remember', 'could', 'bunch', 'things']),\n",
       "       list([]),\n",
       "       list(['turn', 'staff', 'extremely', 'upset', 'one', 'wire', 'services', 'nigeria', 'already', 'write', 'story', 'speech', 'already', 'print', 'city', 'across', 'unite', 'state', 'america', 'print', 'monterey', 'check']),\n",
       "       list([]),\n",
       "       list(['story', 'begin', 'former', 'vice', 'president', 'al', 'gore', 'announce', 'nigeria', 'yesterday', 'quote', 'wife', 'tipper', 'open', 'low', 'cost', 'family', 'restaurant']),\n",
       "       list([]), list(['name', 'shoney', 'running']), list([]),\n",
       "       list(['could', 'get', 'back', 'u', 'soil', 'david', 'letterman', 'jay', 'leno', 'already', 'start', 'one', 'big', 'white', 'chef', 'hat', 'tipper', 'saying', 'one', 'burger', 'fries']),\n",
       "       list([]),\n",
       "       list(['three', 'days', 'later', 'get', 'nice', 'long', 'handwritten', 'letter', 'friend', 'partner', 'colleague', 'bill', 'clinton', 'saying', 'congratulations', 'new', 'restaurant', 'al']),\n",
       "       list([]), list(['like', 'celebrate', 'success', 'life']), list([]),\n",
       "       list(['going', 'talk', 'information', 'ecology', 'thinking', 'since', 'plan', 'make', 'lifelong', 'habit', 'coming', 'back', 'ted', 'maybe', 'could', 'talk', 'another', 'time']),\n",
       "       list([]), list(['chris', 'anderson', 'deal']), list([]),\n",
       "       list(['al', 'gore', 'want', 'focus', 'many', 'say', 'would', 'like', 'elaborate', 'climate', 'crisis', 'want', 'start', 'couple', 'going', 'show', 'new', 'image', 'going', 'recapitulate', 'four', 'five', 'slide', 'show', 'update', 'slide', 'show', 'every', 'time', 'give', 'add', 'new', 'image', 'learn', 'every', 'time', 'give', 'like', 'beach', 'combing', 'know', 'every', 'time', 'tide', 'come', 'find', 'shell', 'last', 'two', 'days', 'get', 'new', 'temperature', 'record', 'january', 'unite', 'state', 'america', 'historical', 'average', 'january', 'degree', 'last', 'month', 'degree']),\n",
       "       list(['know', 'want', 'bad', 'news', 'environment', 'kid', 'recapitulation', 'slide', 'going', 'go', 'new', 'material', 'want', 'elaborate', 'couple', 'first', 'project', 'go', 'u', 'contribution', 'global', 'warming', 'business', 'usual', 'efficiency', 'end', 'use', 'electricity', 'end', 'use', 'energy', 'low', 'hanging', 'fruit', 'efficiency', 'conservation', 'cost', 'profit', 'sign', 'wrong', 'negative', 'positive', 'investment', 'pay', 'also', 'effective', 'deflect', 'path']),\n",
       "       list(['car', 'truck', 'talk', 'slideshow', 'want', 'put', 'perspective', 'easy', 'visible', 'target', 'concern', 'global', 'warming', 'pollution', 'come', 'building', 'car', 'truck', 'car', 'truck', 'significant', 'lowest', 'standard', 'world', 'address', 'part', 'puzzle', 'transportation', 'efficiency', 'important', 'car', 'truck', 'renewables', 'current', 'level', 'technological', 'efficiency', 'make', 'difference', 'vinod', 'john', 'doerr', 'others', 'many', 'lot', 'people', 'directly', 'involve', 'wedge', 'going', 'grow', 'rapidly', 'current', 'projection', 'show', 'carbon', 'capture', 'sequestration', 'cc', 'stand', 'likely', 'become', 'killer', 'app', 'enable', 'us', 'continue', 'use', 'fossil', 'fuel', 'way', 'safe', 'quite', 'yet', 'ok']),\n",
       "       list(['reduce', 'emission', 'home', 'expenditure', 'also', 'profitable', 'insulation', 'better', 'design', 'buy', 'green', 'electricity', 'mention', 'automobile', 'buy', 'hybrid', 'use', 'light', 'rail', 'figure', 'option', 'better', 'important']),\n",
       "       list(['green', 'consumer', 'choice', 'everything', 'buy', 'things', 'harsh', 'effect', 'le', 'harsh', 'effect', 'global', 'climate', 'crisis', 'consider', 'make', 'decision', 'live', 'carbon', 'neutral', 'life', 'good', 'branding', 'love', 'get', 'advice', 'help', 'say', 'way', 'connect', 'people', 'easy', 'think', 'really', 'lot', 'us', 'make', 'decision', 'really', 'pretty', 'easy', 'means', 'reduce', 'carbon', 'dioxide', 'emission', 'full', 'range', 'choice', 'make', 'purchase', 'acquire', 'offset', 'remainder', 'completely', 'reduce', 'means', 'elaborate', 'climatecrisis', 'net']),\n",
       "       list(['carbon', 'calculator', 'participant', 'production', 'convene', 'active', 'involvement', 'leading', 'software', 'writer', 'world', 'arcane', 'science', 'carbon', 'calculation', 'construct', 'consumer', 'friendly', 'carbon', 'calculator', 'precisely', 'calculate', 'co', 'emission', 'given', 'option', 'reduce', 'time', 'movie', 'come', 'may', 'update', 'click', 'purchase', 'offset']),\n",
       "       list(['next', 'consider', 'making', 'business', 'carbon', 'neutral', 'us', 'do', 'hard', 'think', 'integrate', 'climate', 'solution', 'innovation', 'whether', 'technology', 'entertainment', 'design', 'architecture', 'community', 'invest', 'sustainably', 'majora', 'mention', 'listen', 'invest', 'money', 'manager', 'compensate', 'basis', 'annual', 'performance', 'ever', 'complain', 'quarterly', 'report', 'ceo', 'management', 'time', 'people', 'pay', 'judge', 'going', 'get', 'pay', 'capital', 'invest', 'base', 'short', 'term', 'return', 'going', 'get', 'short', 'term', 'decision', 'lot', 'say']),\n",
       "       list(['become', 'catalyst', 'change', 'teach', 'others', 'learn', 'talk', 'movie', 'movie', 'version', 'slideshow', 'give', 'two', 'night', 'ago', 'except', 'lot', 'entertain', 'come', 'may', 'many', 'opportunity', 'ensure', 'lot', 'people', 'see', 'consider', 'sending', 'somebody', 'nashville', 'pick', 'well', 'personally', 'going', 'train', 'people', 'give', 'slideshow', 'purpose', 'personal', 'story', 'obviously', 'replace', 'generic', 'approach', 'slide', 'mean', 'link', 'together', 'going', 'conducting', 'course', 'summer', 'group', 'people', 'nominate', 'different', 'folks', 'come', 'give', 'en', 'masse', 'community', 'across', 'country', 'going', 'update', 'slideshow', 'every', 'single', 'week', 'keep', 'right', 'cutting', 'edge', 'working', 'larry', 'lessig', 'somewhere', 'process', 'post', 'tool', 'limited', 'use', 'copyright', 'young', 'people', 'remix', 'way']),\n",
       "       list([]),\n",
       "       list(['anybody', 'get', 'idea', 'ought', 'stay', 'arm', 'length', 'politics', 'mean', 'republican', 'try', 'convince', 'democrat', 'need', 'republican', 'well', 'use', 'bipartisan', 'issue', 'know', 'group', 'really', 'become', 'politically', 'active', 'make', 'democracy', 'work', 'way', 'suppose', 'work', 'support', 'idea', 'cap', 'carbon', 'dioxide', 'emission', 'global', 'warming', 'pollution', 'trading', 'long', 'unite', 'state', 'world', 'system', 'close', 'system', 'become', 'close', 'system', 'u', 'participation', 'everybody', 'board', 'director', 'many', 'people', 'serve', 'board', 'director', 'corporation', 'close', 'system', 'legal', 'liability', 'urge', 'ceo', 'get', 'maximum', 'income', 'reducing', 'trading', 'carbon', 'emission', 'avoid', 'market', 'work', 'solve', 'problem', 'accomplish', 'help', 'mass', 'persuasion', 'campaign', 'start', 'spring', 'change', 'mind', 'american', 'people', 'presently', 'politician', 'permission', 'need', 'do']),\n",
       "       list(['modern', 'country', 'role', 'logic', 'reason', 'longer', 'include', 'mediate', 'wealth', 'power', 'way', 'repetition', 'short', 'hot', 'button', 'second', 'second', 'television', 'ad', 'buy', 'lot', 'ad', 'let', 'brand', 'global', 'warming', 'many', 'suggest', 'like', 'climate', 'crisis', 'instead', 'climate', 'collapse', 'good', 'branding', 'need', 'help', 'somebody', 'say', 'test', 'facing', 'scientist', 'tell', 'whether', 'combination', 'opposable', 'thumb', 'neocortex', 'viable', 'combination']),\n",
       "       list([]), list(['really', 'true']),\n",
       "       list(['say', 'night', 'repeat', 'political', 'issue', 'republican', 'partisan', 'influence', 'us', 'democrat', 'opportunity', 'connect', 'idea', 'bring', 'coherence', 'one']),\n",
       "       list(['appreciate'])], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "talk_sub[talk_sub.talk_id == 1]['transcript'].apply(lambda x: prepare_text(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alternative energy': 0,\n",
       " 'cars': 1,\n",
       " 'climate change': 2,\n",
       " 'culture': 3,\n",
       " 'environment': 4,\n",
       " 'global issues': 5,\n",
       " 'science': 6,\n",
       " 'sustainability': 7,\n",
       " 'technology': 8,\n",
       " 'MacArthur grant': 9,\n",
       " 'design': 10,\n",
       " 'engineering': 11,\n",
       " 'industrial design': 12,\n",
       " 'invention': 13,\n",
       " 'simplicity': 14,\n",
       " 'business': 15,\n",
       " 'corruption': 16,\n",
       " 'economics': 17,\n",
       " 'entrepreneur': 18,\n",
       " 'global development': 19,\n",
       " 'investment': 20,\n",
       " 'military': 21,\n",
       " 'policy': 22,\n",
       " 'politics': 23,\n",
       " 'poverty': 24,\n",
       " 'NASA': 25,\n",
       " 'aircraft': 26,\n",
       " 'flight': 27,\n",
       " 'rocket science': 28,\n",
       " 'art': 29,\n",
       " 'transportation': 30,\n",
       " 'DNA': 31,\n",
       " 'biodiversity': 32,\n",
       " 'biology': 33,\n",
       " 'biotech': 34,\n",
       " 'ecology': 35,\n",
       " 'genetics': 36,\n",
       " 'oceans': 37,\n",
       " 'computers': 38,\n",
       " 'entertainment': 39,\n",
       " 'interface design': 40,\n",
       " 'media': 41,\n",
       " 'music': 42,\n",
       " 'performance': 43,\n",
       " 'software': 44,\n",
       " 'New York': 45,\n",
       " 'architecture': 46,\n",
       " 'cities': 47,\n",
       " 'collaboration': 48,\n",
       " 'death': 49,\n",
       " 'disaster relief': 50,\n",
       " 'interview': 51,\n",
       " 'memory': 52,\n",
       " 'urban planning': 53,\n",
       " 'innovation': 54,\n",
       " 'robots': 55,\n",
       " 'social change': 56,\n",
       " 'disease': 57,\n",
       " 'food': 58,\n",
       " 'health': 59,\n",
       " 'health care': 60,\n",
       " 'obesity': 61,\n",
       " 'Africa': 62,\n",
       " 'animals': 63,\n",
       " 'nature': 64,\n",
       " 'primates': 65,\n",
       " 'cancer': 66,\n",
       " 'wunderkind': 67,\n",
       " 'creativity': 68,\n",
       " 'cognitive science': 69,\n",
       " 'evolution': 70,\n",
       " 'gender': 71,\n",
       " 'love': 72,\n",
       " 'psychology': 73,\n",
       " 'relationships': 74,\n",
       " 'biomimicry': 75,\n",
       " 'fish': 76,\n",
       " 'choice': 77,\n",
       " 'future': 78,\n",
       " 'history': 79,\n",
       " 'philosophy': 80,\n",
       " 'consumerism': 81,\n",
       " 'marketing': 82,\n",
       " 'storytelling': 83,\n",
       " 'communication': 84,\n",
       " 'community': 85,\n",
       " 'faith': 86,\n",
       " 'illusion': 87,\n",
       " 'religion': 88,\n",
       " 'activism': 89,\n",
       " 'film': 90,\n",
       " 'library': 91,\n",
       " 'open-source': 92,\n",
       " 'poetry': 93,\n",
       " 'product design': 94,\n",
       " 'science and art': 95,\n",
       " 'narcotics': 96,\n",
       " 'race': 97,\n",
       " 'parenting': 98,\n",
       " 'statistics': 99,\n",
       " 'Brazil': 100,\n",
       " 'animation': 101,\n",
       " 'peace': 102,\n",
       " 'terrorism': 103,\n",
       " 'war': 104,\n",
       " 'indigenous peoples': 105,\n",
       " 'photography': 106,\n",
       " 'wikipedia': 107,\n",
       " 'aging': 108,\n",
       " 'global commons': 109,\n",
       " 'children': 110,\n",
       " 'education': 111,\n",
       " 'philanthropy': 112,\n",
       " 'astronomy': 113,\n",
       " 'complexity': 114,\n",
       " 'cosmos': 115,\n",
       " 'time': 116,\n",
       " 'universe': 117,\n",
       " 'happiness': 118,\n",
       " 'live music': 119,\n",
       " 'violin': 120,\n",
       " 'youth': 121,\n",
       " 'piano': 122,\n",
       " 'physics': 123,\n",
       " 'materials': 124,\n",
       " 'graphic design': 125,\n",
       " 'typography': 126,\n",
       " 'energy': 127,\n",
       " 'green': 128,\n",
       " 'inequality': 129,\n",
       " 'pollution': 130,\n",
       " 'movies': 131,\n",
       " 'medicine': 132,\n",
       " 'ebola': 133,\n",
       " 'AIDS': 134,\n",
       " 'performance art': 135,\n",
       " 'sports': 136,\n",
       " 'theater': 137,\n",
       " 'map': 138,\n",
       " 'women': 139,\n",
       " 'demo': 140,\n",
       " 'dance': 141,\n",
       " 'teaching': 142,\n",
       " 'evolutionary psychology': 143,\n",
       " 'anthropology': 144,\n",
       " 'language': 145,\n",
       " 'success': 146,\n",
       " 'work': 147,\n",
       " 'Christianity': 148,\n",
       " 'God': 149,\n",
       " 'leadership': 150,\n",
       " 'motivation': 151,\n",
       " 'personal growth': 152,\n",
       " 'potential': 153,\n",
       " 'work-life balance': 154,\n",
       " 'apes': 155,\n",
       " 'intelligence': 156,\n",
       " 'biomechanics': 157,\n",
       " 'online video': 158,\n",
       " 'brain': 159,\n",
       " 'microfinance': 160,\n",
       " 'telecom': 161,\n",
       " 'singer': 162,\n",
       " 'prosthetics': 163,\n",
       " 'ants': 164,\n",
       " 'insects': 165,\n",
       " 'atheism': 166,\n",
       " 'comedy': 167,\n",
       " 'humor': 168,\n",
       " 'exploration': 169,\n",
       " 'travel': 170,\n",
       " 'code': 171,\n",
       " 'Asia': 172,\n",
       " 'Google': 173,\n",
       " 'math': 174,\n",
       " 'visualizations': 175,\n",
       " 'decision-making': 176,\n",
       " 'consciousness': 177,\n",
       " 'guitar': 178,\n",
       " 'vocals': 179,\n",
       " 'cello': 180,\n",
       " 'self': 181,\n",
       " 'china': 182,\n",
       " 'electricity': 183,\n",
       " 'web': 184,\n",
       " 'spoken word': 185,\n",
       " 'composing': 186,\n",
       " 'natural disaster': 187,\n",
       " 'meme': 188,\n",
       " 'museums': 189,\n",
       " 'water': 190,\n",
       " 'AI': 191,\n",
       " 'marine biology': 192,\n",
       " 'women in business': 193,\n",
       " 'microsoft': 194,\n",
       " 'virtual reality': 195,\n",
       " 'Buddhism': 196,\n",
       " 'Moon': 197,\n",
       " 'Planets': 198,\n",
       " 'adventure': 199,\n",
       " 'mining': 200,\n",
       " 'space': 201,\n",
       " 'Bioethics': 202,\n",
       " 'HIV': 203,\n",
       " 'gaming': 204,\n",
       " 'literature': 205,\n",
       " 'markets': 206,\n",
       " 'books': 207,\n",
       " 'sociology': 208,\n",
       " 'violence': 209,\n",
       " 'asteroid': 210,\n",
       " 'humanity': 211,\n",
       " 'solar system': 212,\n",
       " 'human origins': 213,\n",
       " 'paleontology': 214,\n",
       " 'Best of the Web': 215,\n",
       " 'drones': 216,\n",
       " 'solar energy': 217,\n",
       " 'illness': 218,\n",
       " 'law': 219,\n",
       " 'depression': 220,\n",
       " 'mental health': 221,\n",
       " 'suicide': 222,\n",
       " 'mindfulness': 223,\n",
       " 'String theory': 224,\n",
       " 'magic': 225,\n",
       " 'compassion': 226,\n",
       " 'empathy': 227,\n",
       " 'writing': 228,\n",
       " 'play': 229,\n",
       " 'South America': 230,\n",
       " 'world cultures': 231,\n",
       " 'infrastructure': 232,\n",
       " 'bees': 233,\n",
       " 'garden': 234,\n",
       " 'plants': 235,\n",
       " 'ancient world': 236,\n",
       " 'telescopes': 237,\n",
       " 'toy': 238,\n",
       " 'hack': 239,\n",
       " 'news': 240,\n",
       " 'heart health': 241,\n",
       " 'public health': 242,\n",
       " 'big bang': 243,\n",
       " 'quantum physics': 244,\n",
       " 'fungi': 245,\n",
       " 'bacteria': 246,\n",
       " 'microbiology': 247,\n",
       " 'submarine': 248,\n",
       " 'sex': 249,\n",
       " 'society': 250,\n",
       " 'archaeology': 251,\n",
       " 'dinosaurs': 252,\n",
       " 'crime': 253,\n",
       " 'evil': 254,\n",
       " 'prison': 255,\n",
       " 'beauty': 256,\n",
       " 'plastic': 257,\n",
       " 'Vaccines': 258,\n",
       " 'conducting': 259,\n",
       " 'family': 260,\n",
       " 'trees': 261,\n",
       " 'astrobiology': 262,\n",
       " 'extraterrestrial life': 263,\n",
       " 'Emotions': 264,\n",
       " 'introvert': 265,\n",
       " 'personality': 266,\n",
       " 'origami': 267,\n",
       " 'diversity': 268,\n",
       " 'dark matter': 269,\n",
       " 'identity': 270,\n",
       " 'nanoscale': 271,\n",
       " 'television': 272,\n",
       " 'morality': 273,\n",
       " 'geology': 274,\n",
       " 'life': 275,\n",
       " 'presentation': 276,\n",
       " 'democracy': 277,\n",
       " 'smell': 278,\n",
       " 'social media': 279,\n",
       " 'Senses': 280,\n",
       " 'fashion': 281,\n",
       " 'Mars': 282,\n",
       " \"Alzheimer's\": 283,\n",
       " '3d printing': 284,\n",
       " 'goal-setting': 285,\n",
       " 'curiosity': 286,\n",
       " 'programming': 287,\n",
       " 'chemistry': 288,\n",
       " 'shopping': 289,\n",
       " 'bionics': 290,\n",
       " 'body language': 291,\n",
       " 'virus': 292,\n",
       " 'fear': 293,\n",
       " 'birds': 294,\n",
       " 'wind energy': 295,\n",
       " 'extreme sports': 296,\n",
       " 'prediction': 297,\n",
       " 'productivity': 298,\n",
       " 'coral reefs': 299,\n",
       " 'mind': 300,\n",
       " 'Natural resources': 301,\n",
       " 'agriculture': 302,\n",
       " 'india': 303,\n",
       " 'neuroscience': 304,\n",
       " 'money': 305,\n",
       " 'state-building': 306,\n",
       " 'Antarctica': 307,\n",
       " 'Anthropocene': 308,\n",
       " 'Europe': 309,\n",
       " 'data': 310,\n",
       " 'sight': 311,\n",
       " 'journalism': 312,\n",
       " 'Internet': 313,\n",
       " 'government': 314,\n",
       " 'Sun': 315,\n",
       " 'men': 316,\n",
       " 'advertising': 317,\n",
       " 'sanitation': 318,\n",
       " 'Homelessness': 319,\n",
       " 'charter for compassion': 320,\n",
       " 'weather': 321,\n",
       " 'big problems': 322,\n",
       " 'rivers': 323,\n",
       " 'Slavery': 324,\n",
       " 'sexual violence': 325,\n",
       " 'trafficking': 326,\n",
       " 'Egypt': 327,\n",
       " 'feminism': 328,\n",
       " 'Autism spectrum disorder': 329,\n",
       " 'Science Fiction': 330,\n",
       " 'botany': 331,\n",
       " 'mission blue': 332,\n",
       " 'friendship': 333,\n",
       " 'nuclear weapons': 334,\n",
       " 'oil': 335,\n",
       " 'novel': 336,\n",
       " 'iraq': 337,\n",
       " 'Islam': 338,\n",
       " 'monkeys': 339,\n",
       " 'Iran': 340,\n",
       " 'Middle East': 341,\n",
       " 'sound': 342,\n",
       " 'PTSD': 343,\n",
       " 'population': 344,\n",
       " 'manufacturing': 345,\n",
       " 'Gender equality': 346,\n",
       " 'bullying': 347,\n",
       " 'trust': 348,\n",
       " 'sleep': 349,\n",
       " 'Foreign Policy': 350,\n",
       " 'Surgery': 351,\n",
       " 'medical research': 352,\n",
       " 'protests': 353,\n",
       " 'deextinction': 354,\n",
       " 'disability': 355,\n",
       " 'nuclear energy': 356,\n",
       " 'driverless cars': 357,\n",
       " 'crowdsourcing': 358,\n",
       " 'Brand': 359,\n",
       " 'speech': 360,\n",
       " 'failure': 361,\n",
       " 'security': 362,\n",
       " 'pain': 363,\n",
       " 'Blindness': 364,\n",
       " 'Gender spectrum': 365,\n",
       " 'glacier': 366,\n",
       " 'mobility': 367,\n",
       " 'LGBT': 368,\n",
       " 'public spaces': 369,\n",
       " 'encryption': 370,\n",
       " 'nonviolence': 371,\n",
       " 'pharmaceuticals': 372,\n",
       " 'molecular biology': 373,\n",
       " 'Surveillance': 374,\n",
       " 'behavioral economics': 375,\n",
       " 'medical imaging': 376,\n",
       " 'physiology': 377,\n",
       " 'Human body': 378,\n",
       " 'pregnancy': 379,\n",
       " 'synthetic biology': 380,\n",
       " 'justice system': 381,\n",
       " 'hearing': 382,\n",
       " 'jazz': 383,\n",
       " 'Nobel prize': 384,\n",
       " 'finance': 385,\n",
       " 'algorithm': 386,\n",
       " 'Guns': 387,\n",
       " 'exercise': 388,\n",
       " 'conservation': 389,\n",
       " 'immigration': 390,\n",
       " 'privacy': 391,\n",
       " 'microbes': 392,\n",
       " 'machine learning': 393,\n",
       " 'skateboarding': 394,\n",
       " 'forensics': 395,\n",
       " 'painting': 396,\n",
       " 'pandemic': 397,\n",
       " 'meditation': 398,\n",
       " 'Syria': 399,\n",
       " 'Transgender': 400,\n",
       " 'student': 401,\n",
       " 'farming': 402,\n",
       " 'blockchain': 403,\n",
       " 'cryptocurrency': 404,\n",
       " 'Debate': 405,\n",
       " 'Criminal Justice': 406,\n",
       " 'cloud': 407,\n",
       " 'refugees': 408,\n",
       " 'street art': 409,\n",
       " 'Addiction': 410,\n",
       " 'CRISPR': 411,\n",
       " 'vulnerability': 412,\n",
       " 'capitalism': 413,\n",
       " 'grammar': 414,\n",
       " 'discovery': 415,\n",
       " 'Audacious Project': 416,\n",
       " 'resources': 417,\n",
       " 'urban': 418,\n",
       " 'biosphere': 419,\n",
       " 'epidemiology': 420,\n",
       " 'funny': 421,\n",
       " 'cooperation': 422}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx = {}\n",
    "i=0\n",
    "talktags = [taglist.split(',') for taglist in talk_meta['tags'].values]\n",
    "for taglist in talktags:\n",
    "    for tag in taglist:\n",
    "        if tag not in tag2idx and not 'ted' in tag.lower():\n",
    "            tag2idx[tag] = i\n",
    "            i += 1\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pickle\n",
    "\n",
    "with open(path.expanduser('~/work/data/talk_pre.pkl'), \"rb\") as input_file:\n",
    "    talk_pre = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/93/c6011037f24e3106d13f3be55297bf84ece2bf15b278cc4776339dc52db5/gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2MB 138kB/s eta 0:00:01    |█████▋                          | 4.2MB 165kB/s eta 0:02:01     |███████▎                        | 5.5MB 140kB/s eta 0:02:14\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/09/735f2786dfac9bbf39d244ce75c0313d27d4962e71e0774750dc809f2395/smart_open-1.9.0.tar.gz (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 156kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.3.1)\n",
      "Collecting boto>=2.32 (from smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 128kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/3c/00bc2a81bb9d9a0d708bb7f63d63fbb92aef63bc459b01bf317431ca7220/boto3-1.12.5-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 152kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 190kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.16.0,>=1.15.5 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/2b/f290e79241a6b07ab955a65d0395128bb300711ef37589a01696de1c5b5f/botocore-1.15.5-py2.py3-none-any.whl (5.9MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9MB 159kB/s eta 0:00:01     |████████████████████▏           | 3.7MB 116kB/s eta 0:00:19\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10 (from botocore<1.16.0,>=1.15.5->boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "\u001b[K     |████████████████████████████████| 552kB 153kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.5->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-1.9.0-cp37-none-any.whl size=73088 sha256=b01c5908cf40f0465b1ebb56f8b0fedcd33881387e03c30b53470dd8c6601298\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/ab/10/93/5cff86f5b721d77edaecc29959b1c60d894be1f66d91407d28\n",
      "Successfully built smart-open\n",
      "Installing collected packages: boto, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto-2.49.0 boto3-1.12.5 botocore-1.15.5 docutils-0.15.2 gensim-3.8.1 jmespath-0.9.4 s3transfer-0.3.3 smart-open-1.9.0\n"
     ]
    }
   ],
   "source": [
    "# !sudo apt update -y\n",
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pasquale/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pasquale/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import nltk\n",
    "import gensim\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                             content  target  \\\n0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n\n            target_names  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>target</th>\n      <th>target_names</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n      <td>14</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                          content  target\ntarget_names                             \nalt.atheism                   480     480\ncomp.graphics                 584     584\ncomp.os.ms-windows.misc       591     591\ncomp.sys.ibm.pc.hardware      590     590\ncomp.sys.mac.hardware         578     578\ncomp.windows.x                593     593\nmisc.forsale                  585     585\nrec.autos                     594     594\nrec.motorcycles               598     598\nrec.sport.baseball            597     597\nrec.sport.hockey              600     600\nsci.crypt                     595     595\nsci.electronics               591     591\nsci.med                       594     594\nsci.space                     593     593\nsoc.religion.christian        599     599\ntalk.politics.guns            546     546\ntalk.politics.mideast         564     564\ntalk.politics.misc            465     465\ntalk.religion.misc            377     377",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>target_names</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alt.atheism</th>\n      <td>480</td>\n      <td>480</td>\n    </tr>\n    <tr>\n      <th>comp.graphics</th>\n      <td>584</td>\n      <td>584</td>\n    </tr>\n    <tr>\n      <th>comp.os.ms-windows.misc</th>\n      <td>591</td>\n      <td>591</td>\n    </tr>\n    <tr>\n      <th>comp.sys.ibm.pc.hardware</th>\n      <td>590</td>\n      <td>590</td>\n    </tr>\n    <tr>\n      <th>comp.sys.mac.hardware</th>\n      <td>578</td>\n      <td>578</td>\n    </tr>\n    <tr>\n      <th>comp.windows.x</th>\n      <td>593</td>\n      <td>593</td>\n    </tr>\n    <tr>\n      <th>misc.forsale</th>\n      <td>585</td>\n      <td>585</td>\n    </tr>\n    <tr>\n      <th>rec.autos</th>\n      <td>594</td>\n      <td>594</td>\n    </tr>\n    <tr>\n      <th>rec.motorcycles</th>\n      <td>598</td>\n      <td>598</td>\n    </tr>\n    <tr>\n      <th>rec.sport.baseball</th>\n      <td>597</td>\n      <td>597</td>\n    </tr>\n    <tr>\n      <th>rec.sport.hockey</th>\n      <td>600</td>\n      <td>600</td>\n    </tr>\n    <tr>\n      <th>sci.crypt</th>\n      <td>595</td>\n      <td>595</td>\n    </tr>\n    <tr>\n      <th>sci.electronics</th>\n      <td>591</td>\n      <td>591</td>\n    </tr>\n    <tr>\n      <th>sci.med</th>\n      <td>594</td>\n      <td>594</td>\n    </tr>\n    <tr>\n      <th>sci.space</th>\n      <td>593</td>\n      <td>593</td>\n    </tr>\n    <tr>\n      <th>soc.religion.christian</th>\n      <td>599</td>\n      <td>599</td>\n    </tr>\n    <tr>\n      <th>talk.politics.guns</th>\n      <td>546</td>\n      <td>546</td>\n    </tr>\n    <tr>\n      <th>talk.politics.mideast</th>\n      <td>564</td>\n      <td>564</td>\n    </tr>\n    <tr>\n      <th>talk.politics.misc</th>\n      <td>465</td>\n      <td>465</td>\n    </tr>\n    <tr>\n      <th>talk.religion.misc</th>\n      <td>377</td>\n      <td>377</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "data.groupby('target_names').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lem = nltk.stem.WordNetLemmatizer()\n",
    "def prepare_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('from:.+\\n', '', text)\n",
    "    text = re.sub('article-i.d.:.+\\n', '', text)\n",
    "    text = re.sub('nntp-posting-host:.+\\n', '', text)\n",
    "    text = re.sub('organization:.+\\n', '', text)\n",
    "    text = re.sub('x-newsreader:.+\\n', '', text)\n",
    "    text = re.sub('distribution:.+\\n', '', text)\n",
    "    text = re.sub('reply-to:.+\\n', '', text)\n",
    "    text = re.sub('<.+>', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "    text = [lem.lemmatize(w) for w in text]\n",
    "    text = [w for w in text if not(w in ['from', 'subject', 'edu', 'use', 'lines', 'ke'] or w in nltk.corpus.stopwords.words('english')) and len(w)>=3 and '__' not in w]\n",
    "    text = [w for w in text if len(w)>=3]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d16678533602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprepare_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d16678533602>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprepare_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-585950a59e88>\u001b[0m in \u001b[0;36mprepare_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'from'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subject'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'edu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'use'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lines'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ke'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'__'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-585950a59e88>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'from'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subject'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'edu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'use'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lines'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ke'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'__'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     23\u001b[0m         return [\n\u001b[1;32m     24\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         ]\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "data['content'] = data.content.apply(lambda x: prepare_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open('20ng.txt', 'w') as f:\n",
    "     for d in data['content']:\n",
    "            f.write(' '.join(d))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = data['content'].values\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(data_words)\n",
    "id2word.filter_n_most_frequent(10)\n",
    "\n",
    "# corpus = [id2word.doc2bow(text) for text in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text, tokens):\n",
    "    filtered_text = []\n",
    "    for word in text:\n",
    "            filtered_text.append(word)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data.content.apply(lambda x: filter_text(x, id2word.token2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content'] = data.content.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path.expanduser('~/work/data/20NG.txt'),\"w+\") \n",
    "for text in data['content']:\n",
    "    f.write(text+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = pd.read_csv('~/work/data/wikipedia_utf8_filtered_20pageviews.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>wikipedia-23885690</td>\n",
       "      <td>Research Design and Standards Organization  T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>wikipedia-23885928</td>\n",
       "      <td>The Death of Bunny Munro  The Death of Bunny ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>wikipedia-23886057</td>\n",
       "      <td>Management of prostate cancer  Treatment for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>wikipedia-23886425</td>\n",
       "      <td>Cheetah reintroduction in India  Reintroducti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>wikipedia-23886491</td>\n",
       "      <td>Langtang National Park  The Langtang National...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0                                                  1\n",
       "0  wikipedia-23885690   Research Design and Standards Organization  T...\n",
       "1  wikipedia-23885928   The Death of Bunny Munro  The Death of Bunny ...\n",
       "2  wikipedia-23886057   Management of prostate cancer  Treatment for ...\n",
       "3  wikipedia-23886425   Cheetah reintroduction in India  Reintroducti...\n",
       "4  wikipedia-23886491   Langtang National Park  The Langtang National..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def simple_prepare_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "    text = [lem.lemmatize(w) for w in text]\n",
    "    text = [w for w in text if len(w)>=3]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref[1] = ref[1].apply(lambda x: simple_prepare_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463819"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Progress 99.9997843986555 % - Duration per doc: 4.185199737548828 ms'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = open(path.expanduser('~/work/data/wiki.txt'),\"w+\") \n",
    "i = 0\n",
    "total = len(ref)\n",
    "for text in ref[1]:\n",
    "    clear_output(wait=True)\n",
    "    start = time.time()\n",
    "    text = simple_prepare_text(text)\n",
    "    f.write(text+'\\n')\n",
    "    dur = time.time() - start\n",
    "    display('Progress '+ str(100*i/total) + ' % - Duration per doc: '+ str(dur*1000) + ' ms')\n",
    "    i+=1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.65158333333333 mins\n"
     ]
    }
   ],
   "source": [
    "print(str(5*463819/1000/60) + ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(path.expanduser('~/work/data/sw.txt'),\"w+\") \n",
    "\n",
    "for w in simple_prepare_text(' '.join(nltk.corpus.stopwords.words('english'))).split():\n",
    "#     s|\\<the\\>||g\n",
    "    f.write('s|\\<'+w+'\\>||g\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from os import path\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = nltk.stem.WordNetLemmatizer()\n",
    "def prepare_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "    text = [lem.lemmatize(w) for w in text]\n",
    "    text = [w for w in text if w not in nltk.corpus.stopwords.words('english')]\n",
    "    text = [w for w in text if len(w)>=3]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(path.expanduser('~/work/data/asrael.txt'),\"r\") \n",
    "f2 = open(path.expanduser('~/work/data/asrael_cleaned.txt'),\"w\") \n",
    "\n",
    "i = 0\n",
    "total = 125516\n",
    "for line in f1:\n",
    "    clear_output(wait=True)\n",
    "    start = time.time()\n",
    "    text = ' '.join(prepare_text(line))\n",
    "    f2.write(text+'\\n')\n",
    "    dur = time.time() - start\n",
    "    display('Progress '+ str(100*i/total) + ' % - Duration per doc: '+ str(dur*1000) + ' ms')\n",
    "    i+=1\n",
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}