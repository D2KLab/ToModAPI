{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tokens(x, tok2remove):\n",
    "    return ' '.join(['' if t in tok2remove else t for t in x.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'know best human power flight one thing get going sort things working youngster interest model airplane ornithopter autogyro helicopter glider power plane indoor model outdoor model everything thought lot fun wonder people share enthusiasm navy pilot training college get sailplane flying power plane flying consider sailplane sort hobby fun get tangle great professor type convince everybody else field good way get really deep science going field weather modification although getting ph aeronautics weather modification subject getting start graduate student could go around various talks given hitchhiker ride east coast everybody would talk professional field hate communicate result get absolutely unique background field start company research weather modification anybody lot things go start aerovironment employee one two three sort fumble along try get interest project airdynamisis like want work aerospace company big many year project small project company slowly grow thing excite suddenly get interest human power airplane make make loan friend dollar guarantee money bank need need money starting company company succeed pay money back guarantor note debt notice kramer prize human power flight around years time pound exchange rate dollar suddenly interest human power flight way approach first thinking ways make plane like england succeed give figure nah simple easy way get vacation trip studying bird flight fun watch bird soaring around circle measure time estimate bank angle immediately figure speed turning radius could car driving along vacation trip three son young son helping ridicule whole thing much begin thinking bird go around airplane would hang glider would fly plane idea gossamer condor type airplane quickly emerge logical one thought first place one keep weight pound weigh let size swell like hang glider three times span three times cord third speed third power good bicyclist put power work prize year later lot flying lot experiment lot things work one work plane keep getting little better little better get good pilot brian allen operate finally succeed unfortunately dollar spend project help retire debt fortunately henry kramer put prize one mile flight put new prize flying english channel mile thought would take another years somebody win realize clean gossamer condor little bit power fly would decrease little bit decrease power require little pilot fly much longer period time brian allen able miraculous flight get gossamer albatross across english channel pound dollar prize expense pay debt handle everything fine turn giving plane museum worth much debt five years six years pay one third income tax good economic reason project well project do entirely economic reason involve human power flight since prize sure start thinking various things immediately begin making solar power plane felt solar power going important country world want small funding government decrease government try thought solar power plane really make sense could would get lot publicity solar power maybe help field project continue succeed get project aviation mechanical things ground devices going get prize lindbergh foundation annual prize prepare paper collect vary thought vary interest years one chance focus really important surprise realize importance environmental issue charles lindbergh devote last third life prepare paper lot good thought back space traveler come visit earth every years thousand visit would see thing every time little difference earth last time coming round right suddenly huge change environment concentration people unbelievable amount change want well one big change years ago begin using coal underground lot pollution years ago begin getting gasoline underground lot pollution gasoline consumption production reach limit ten years go wonder going happen transportation want show slide slide think important one see ever show nature versus humans go year see weight air land vertebrate humans muskrat giraffe bird red line go humans livestock pet portion green line go wild nature portion humans livestock pet percent total world mass vertebrate land air know future hold going get lower percentage ten thousand years ago humans livestock pet even one tenth one percent even visible curve percent think show human domination earth give talk remarkable high school student summer ask ask question give talk ask question population earth population earth going age parent never really never really thought think population earth would equilibrium could continue form little group fighting leave two hours later saying billion people clue get billion think right serious problem rachel carson thinking come silent spring way back solar manifesto hermann scheer germany claim energy earth derive every country solar energy water need dig chemical things much efficiently let next slide summarize billion years unique sphere chance paint thin covering life complex probable wonderful fragile suddenly humans recently arrive species longer subject check balance inherent nature grow population technology intelligence position terrible power wield paintbrush charge frightening painting every years last one show earth time flag right trilobite dinosaur triangle get civilization tv traffic jam idea come next use robotic natural cockroach future little warning two week drawing do actually first project contract aerovironment robotic cockroach frightening well slide time go stop environmental program focus really serious energy problem future produce product company develop impact car general motor make ev get air resource board regulation stimulate electric car since come apart do lot things small drone airplane helios first video narrator wingspan foot make larger boeing designer attention detail construction give helios structure flexibility strength deal turbulence encounter atmosphere enable easily ride air current slide along ocean wave paul maccready wings could touch together top break think narrator helios begin process turning back sun maximize power solar array sky get dark outside air temperature drop minus degree fahrenheit environmentally hostile segment helios journey go without notice except record specially design data acquisition system associate sensor approaching peak radar altitude foot helios standing top percent earth atmosphere foot higher previous world altitude record hold sr blackbird pm plane many purpose aim communications fly slowly stay foot eventually able stay day night day night six month time acting like synchronous satellite ten mile earth let next video show end spectrum narrator tiny airplane av pointer serve surveillance effect pair roving eyeglasses cutting edge example miniaturization lead operator remote vehicle convenient carry assemble launch hand battery power silent rarely notice send high resolution video picture back operator board gps navigate autonomously rugged enough self land without damage pm okay let next plane widely use military operations let next video alan alda get get get head going end visit paul maccready flying circus meeting son tyler two brother help build gossamer condor years ago tyler maccready chase like hours aa get bore father project invent extraordinary little plane tm control putting lift one side wing aa call walkalong glider never see anything like old invent tm oh tm something like pm tyler show walkalong tm right get couple gift bag one first things production version seem dive little bit would suggest bend wing tip little bit try flying give demonstration works idea soar lift body like seagull soaring cliff wind come go cliff walk air go around body go keep glider position current launch difficult part get hold high head start walking forward let go control like also like say video turn left right putting lift one wing another oops going right turn okay one left turn anyway control wherever want hours fun longer production real collector item want show get video running yeah example little video surveillance flying around party last night see fly around spy anybody want going bring airplane worry hitting people thought would little bit gentle yeah invention right'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_tokens(data1[data1['transcript'].str.contains('airdynamisis')].transcript.values[0],{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease               \u001b[0m\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease             \u001b[33m\n",
      "Reading package lists... Done\u001b[0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.17.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (1.10.15)\n",
      "Requirement already satisfied: boto>=2.32 in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.15 in /opt/conda/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.15)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.15->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from botocore<1.14.0,>=1.13.15->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (2.2.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy) (41.0.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from spacy) (0.23)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.17.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.32.2)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (7.2.0)\n",
      "Collecting pyLDAvis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from pyLDAvis) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.9.2 in /opt/conda/lib/python3.7/site-packages (from pyLDAvis) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.18.0 in /opt/conda/lib/python3.7/site-packages (from pyLDAvis) (1.3.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from pyLDAvis) (0.25.1)\n",
      "Requirement already satisfied: joblib>=0.8.4 in /opt/conda/lib/python3.7/site-packages (from pyLDAvis) (0.13.2)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from pyLDAvis) (2.10.1)\n",
      "Requirement already satisfied: numexpr in /opt/conda/lib/python3.7/site-packages (from pyLDAvis) (2.6.9)\n",
      "Collecting pytest (from pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/16/f6dec5178f5f4141e80dfc4812a9aba88f5f29ca881f174ab1851181d016/pytest-5.2.2-py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future (from pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting funcy (from pyLDAvis)\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/3a/fc8323f913e8a9c6f33f7203547f8a2171223da5ed965f2541dafb10aa09/funcy-1.13-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2019.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.17.0->pyLDAvis) (2.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from pytest->pyLDAvis) (19.1.0)\n",
      "Collecting py>=1.5.0 (from pytest->pyLDAvis)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/bc/394ad449851729244a97857ee14d7cba61ddb268dce3db538ba2f2ba1f0f/py-1.8.0-py2.py3-none-any.whl (83kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 5.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest->pyLDAvis) (0.23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting atomicwrites>=1.0 (from pytest->pyLDAvis)\n",
      "  Downloading https://files.pythonhosted.org/packages/52/90/6155aa926f43f2b2a22b01be7241be3bfd1ceaf7d0b3267213e8127d41f4/atomicwrites-1.3.0-py2.py3-none-any.whl\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest->pyLDAvis)\n",
      "  Downloading https://files.pythonhosted.org/packages/92/c7/48439f7d5fd6bddb4c04b850bb862b42e3e2b98570040dfaf68aedd8114b/pluggy-0.13.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest->pyLDAvis) (7.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas>=0.17.0->pyLDAvis) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest->pyLDAvis) (2.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pyLDAvis) (0.6.0)\n",
      "Building wheels for collected packages: pyLDAvis, future\n",
      "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97712 sha256=0f2caa5b21f947f7c8558c26f74aa5b2d95251f107d080473b1c1afaea79b28a\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491057 sha256=08ac082fc575caecadb8b8cd767029cb9466cad91fadf803c3a3d97b02871a94\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built pyLDAvis future\n",
      "Installing collected packages: py, atomicwrites, pluggy, pytest, future, funcy, pyLDAvis\n",
      "Successfully installed atomicwrites-1.3.0 funcy-1.13 future-0.18.2 pluggy-0.13.0 py-1.8.0 pyLDAvis-2.1.2 pytest-5.2.2\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update -y\n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install spacy\n",
    "!pip install pyLDAvis\n",
    "# !sudo apt-get install default-jre -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.expanduser('~/work/data/data.pkl'), \"rb\") as input_file:\n",
    "    data1 = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>ratings</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>good morning great blow away whole thing fact ...</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>4553</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>1164</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>60</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 19645}, {...</td>\n",
       "      <td>[{'id': 865, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Author/educator</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>47227110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>thank much chris truly great honor opportunity...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>265</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>977</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>43</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 544}, {'i...</td>\n",
       "      <td>[{'id': 243, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>3200520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>hello voice mail old friend call tech support ...</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>124</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1286</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140739200</td>\n",
       "      <td>26</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>David Pogue: Simplicity sells</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 964}, {'i...</td>\n",
       "      <td>[{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>1636292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>today happy hear sustainable development save ...</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "      <td>200</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>1116</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140912000</td>\n",
       "      <td>35</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>Majora Carter: Greening the ghetto</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 3, 'name': 'Courageous', 'count': 760}...</td>\n",
       "      <td>[{'id': 1041, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Activist for environmental justice</td>\n",
       "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>1697550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>years ago take task teach global development s...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "      <td>593</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>1190</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140566400</td>\n",
       "      <td>48</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>Hans Rosling: The best stats you've ever seen</td>\n",
       "      <td>1</td>\n",
       "      <td>1151440680</td>\n",
       "      <td>[{'id': 9, 'name': 'Ingenious', 'count': 3202}...</td>\n",
       "      <td>[{'id': 2056, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Global health expert; data visionary</td>\n",
       "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>12005869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  good morning great blow away whole thing fact ...   \n",
       "1  thank much chris truly great honor opportunity...   \n",
       "2  hello voice mail old friend call tech support ...   \n",
       "3  today happy hear sustainable development save ...   \n",
       "4  years ago take task teach global development s...   \n",
       "\n",
       "                                                 url  comments  \\\n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...      4553   \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...       265   \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...       124   \n",
       "3  https://www.ted.com/talks/majora_carter_s_tale...       200   \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...       593   \n",
       "\n",
       "                                         description  duration    event  \\\n",
       "0  Sir Ken Robinson makes an entertaining and pro...      1164  TED2006   \n",
       "1  With the same humor and humanity he exuded in ...       977  TED2006   \n",
       "2  New York Times columnist David Pogue takes aim...      1286  TED2006   \n",
       "3  In an emotionally charged talk, MacArthur-winn...      1116  TED2006   \n",
       "4  You've never seen data presented like this. Wi...      1190  TED2006   \n",
       "\n",
       "    film_date  languages   main_speaker  \\\n",
       "0  1140825600         60   Ken Robinson   \n",
       "1  1140825600         43        Al Gore   \n",
       "2  1140739200         26    David Pogue   \n",
       "3  1140912000         35  Majora Carter   \n",
       "4  1140566400         48   Hans Rosling   \n",
       "\n",
       "                                            name  num_speaker  published_date  \\\n",
       "0      Ken Robinson: Do schools kill creativity?            1      1151367060   \n",
       "1           Al Gore: Averting the climate crisis            1      1151367060   \n",
       "2                  David Pogue: Simplicity sells            1      1151367060   \n",
       "3             Majora Carter: Greening the ghetto            1      1151367060   \n",
       "4  Hans Rosling: The best stats you've ever seen            1      1151440680   \n",
       "\n",
       "                                             ratings  \\\n",
       "0  [{'id': 7, 'name': 'Funny', 'count': 19645}, {...   \n",
       "1  [{'id': 7, 'name': 'Funny', 'count': 544}, {'i...   \n",
       "2  [{'id': 7, 'name': 'Funny', 'count': 964}, {'i...   \n",
       "3  [{'id': 3, 'name': 'Courageous', 'count': 760}...   \n",
       "4  [{'id': 9, 'name': 'Ingenious', 'count': 3202}...   \n",
       "\n",
       "                                       related_talks  \\\n",
       "0  [{'id': 865, 'hero': 'https://pe.tedcdn.com/im...   \n",
       "1  [{'id': 243, 'hero': 'https://pe.tedcdn.com/im...   \n",
       "2  [{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "3  [{'id': 1041, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "4  [{'id': 2056, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "\n",
       "                     speaker_occupation  \\\n",
       "0                       Author/educator   \n",
       "1                      Climate advocate   \n",
       "2                  Technology columnist   \n",
       "3    Activist for environmental justice   \n",
       "4  Global health expert; data visionary   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['children', 'creativity', 'culture', 'dance',...   \n",
       "1  ['alternative energy', 'cars', 'climate change...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "3  ['MacArthur grant', 'activism', 'business', 'c...   \n",
       "4  ['Africa', 'Asia', 'Google', 'demo', 'economic...   \n",
       "\n",
       "                             title     views  \n",
       "0      Do schools kill creativity?  47227110  \n",
       "1      Averting the climate crisis   3200520  \n",
       "2                 Simplicity sells   1636292  \n",
       "3              Greening the ghetto   1697550  \n",
       "4  The best stats you've ever seen  12005869  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['good', 'morning', 'great', 'blow', 'away', 'whole', 'thing', 'fact', 'leaving', 'three', 'theme', 'running', 'conference', 'relevant', 'want', 'talk', 'one', 'extraordinary', 'evidence', 'human', 'creativity', 'presentation', 'people', 'variety', 'range', 'second', 'put', 'us', 'place', 'idea', 'going', 'happen', 'terms', 'future', 'idea', 'may', 'play', 'interest', 'education', 'actually', 'find', 'everybody', 'interest', 'education', 'find', 'interest', 'dinner', 'party', 'say', 'work', 'education', 'actually', 'often', 'dinner', 'party', 'frankly', 'work', 'education', 'ask', 'never', 'ask', 'back', 'curiously', 'strange', 'say', 'somebody', 'know', 'say', 'say', 'work', 'education', 'see', 'blood', 'run', 'face', 'like', 'oh', 'god', 'know', 'one', 'night', 'week', 'ask', 'education', 'pin', 'wall', 'one', 'things', 'go', 'deep', 'people', 'right', 'like', 'religion', 'money', 'things', 'big', 'interest', 'education', 'think', 'huge', 'vest', 'interest', 'partly', 'education', 'mean', 'take', 'us', 'future', 'grasp', 'think', 'child', 'starting', 'school', 'year', 'retire', 'nobody', 'clue', 'despite', 'expertise', 'parade', 'past', 'four', 'days', 'world', 'look', 'like', 'five', 'years', 'time', 'yet', 'mean', 'educate', 'think', 'extraordinary', 'third', 'part', 'agree', 'nonetheless', 'really', 'extraordinary', 'capacity', 'child', 'capacity', 'innovation', 'mean', 'sirena', 'last', 'night', 'marvel', 'seeing', 'could', 'exceptional', 'think', 'speak', 'exceptional', 'whole', 'childhood', 'person', 'extraordinary', 'dedication', 'found', 'talent', 'contention', 'kid', 'tremendous', 'talent', 'squander', 'pretty', 'ruthlessly', 'want', 'talk', 'education', 'want', 'talk', 'creativity', 'contention', 'creativity', 'important', 'education', 'literacy', 'treat', 'status', 'thank', 'way', 'thank', 'much', 'minutes', 'left', 'well', 'born', 'hear', 'great', 'story', 'recently', 'love', 'telling', 'little', 'girl', 'drawing', 'lesson', 'six', 'back', 'drawing', 'teacher', 'say', 'girl', 'hardly', 'ever', 'pay', 'attention', 'drawing', 'lesson', 'teacher', 'fascinate', 'go', 'say', 'drawing', 'girl', 'say', 'drawing', 'picture', 'god', 'teacher', 'say', 'nobody', 'know', 'god', 'look', 'like', 'girl', 'say', 'minute', 'son', 'four', 'england', 'actually', 'four', 'everywhere', 'honest', 'strict', 'wherever', 'go', 'four', 'year', 'nativity', 'play', 'remember', 'story', 'big', 'big', 'story', 'mel', 'gibson', 'sequel', 'may', 'see', 'nativity', 'ii', 'james', 'get', 'part', 'joseph', 'thrill', 'consider', 'one', 'lead', 'parts', 'place', 'cram', 'full', 'agent', 'shirt', 'james', 'robinson', 'joseph', 'speak', 'know', 'bit', 'three', 'king', 'come', 'come', 'bearing', 'gift', 'gold', 'frankincense', 'myrrh', 'really', 'happen', 'sitting', 'think', 'go', 'sequence', 'talk', 'little', 'boy', 'afterward', 'say', 'ok', 'say', 'yeah', 'wrong', 'switch', 'three', 'boy', 'come', 'four', 'year', 'old', 'tea', 'towel', 'head', 'put', 'box', 'first', 'boy', 'say', 'bring', 'gold', 'second', 'boy', 'say', 'bring', 'myrrh', 'third', 'boy', 'say', 'frank', 'sent', 'things', 'common', 'kid', 'take', 'chance', 'know', 'go', 'right', 'frighten', 'wrong', 'mean', 'say', 'wrong', 'thing', 'creative', 'know', 'prepare', 'wrong', 'never', 'come', 'anything', 'original', 'prepare', 'wrong', 'time', 'get', 'adult', 'kid', 'lost', 'capacity', 'become', 'frighten', 'wrong', 'run', 'company', 'like', 'stigmatize', 'mistake', 'running', 'national', 'education', 'system', 'mistake', 'worst', 'thing', 'make', 'result', 'educate', 'people', 'creative', 'capacity', 'picasso', 'say', 'say', 'child', 'born', 'artist', 'problem', 'remain', 'artist', 'grow', 'believe', 'passionately', 'grow', 'creativity', 'grow', 'rather', 'get', 'educate', 'live', 'stratford', 'avon', 'five', 'years', 'ago', 'fact', 'move', 'stratford', 'los', 'angeles', 'imagine', 'seamless', 'transition', 'actually', 'live', 'place', 'call', 'snitterfield', 'outside', 'stratford', 'shakespeare', 'father', 'born', 'strike', 'new', 'thought', 'think', 'shakespeare', 'father', 'think', 'shakespeare', 'child', 'shakespeare', 'seven', 'never', 'thought', 'mean', 'seven', 'point', 'somebody', 'english', 'class', 'annoying', 'would', 'must', 'try', 'hard', 'sent', 'bed', 'dad', 'know', 'shakespeare', 'go', 'bed', 'put', 'pencil', 'stop', 'speaking', 'like', 'confuse', 'everybody', 'anyway', 'move', 'stratford', 'los', 'angeles', 'want', 'say', 'word', 'transition', 'son', 'want', 'come', 'get', 'two', 'kid', 'daughter', 'want', 'come', 'los', 'angeles', 'love', 'girlfriend', 'england', 'love', 'life', 'sarah', 'know', 'month', 'mind', 'fourth', 'anniversary', 'long', 'time', 'really', 'upset', 'plane', 'say', 'never', 'find', 'another', 'girl', 'like', 'sarah', 'rather', 'please', 'frankly', 'main', 'reason', 'leaving', 'country', 'something', 'strike', 'move', 'america', 'travel', 'around', 'world', 'every', 'education', 'system', 'earth', 'hierarchy', 'subject', 'every', 'one', 'matter', 'go', 'think', 'would', 'otherwise', 'top', 'mathematics', 'language', 'humanities', 'bottom', 'arts', 'everywhere', 'earth', 'pretty', 'much', 'every', 'system', 'hierarchy', 'within', 'arts', 'art', 'music', 'normally', 'given', 'higher', 'status', 'school', 'drama', 'dance', 'education', 'system', 'planet', 'teach', 'dance', 'everyday', 'child', 'way', 'teach', 'mathematics', 'think', 'rather', 'important', 'think', 'math', 'important', 'dance', 'child', 'dance', 'time', 'allow', 'body', 'miss', 'meeting', 'truthfully', 'happen', 'child', 'grow', 'start', 'educate', 'progressively', 'waist', 'focus', 'head', 'slightly', 'one', 'side', 'visit', 'education', 'alien', 'say', 'public', 'education', 'think', 'conclude', 'look', 'output', 'really', 'succeed', 'everything', 'get', 'brownie', 'point', 'winner', 'think', 'conclude', 'whole', 'purpose', 'public', 'education', 'throughout', 'world', 'produce', 'university', 'professor', 'people', 'come', 'top', 'use', 'one', 'like', 'university', 'professor', 'know', 'hold', 'high', 'water', 'mark', 'human', 'achievement', 'form', 'life', 'another', 'form', 'life', 'rather', 'curious', 'say', 'affection', 'something', 'curious', 'professor', 'experience', 'typically', 'live', 'head', 'live', 'slightly', 'one', 'side', 'disembody', 'know', 'kind', 'literal', 'way', 'look', 'upon', 'body', 'form', 'transport', 'head', 'way', 'getting', 'head', 'meeting', 'want', 'real', 'evidence', 'body', 'experience', 'get', 'along', 'residential', 'conference', 'senior', 'academic', 'pop', 'discotheque', 'final', 'night', 'see', 'grow', 'men', 'woman', 'writhe', 'uncontrollably', 'beat', 'waiting', 'end', 'go', 'home', 'write', 'paper', 'education', 'system', 'predicate', 'idea', 'academic', 'ability', 'reason', 'around', 'world', 'public', 'system', 'education', 'really', 'th', 'century', 'come', 'meet', 'need', 'industrialism', 'hierarchy', 'root', 'two', 'idea', 'number', 'one', 'useful', 'subject', 'work', 'top', 'probably', 'steer', 'benignly', 'away', 'things', 'school', 'kid', 'things', 'like', 'grounds', 'would', 'never', 'get', 'job', 'right', 'music', 'going', 'musician', 'art', 'artist', 'benign', 'advice', 'profoundly', 'mistake', 'whole', 'world', 'engulf', 'revolution', 'second', 'academic', 'ability', 'really', 'come', 'dominate', 'view', 'intelligence', 'university', 'design', 'system', 'image', 'think', 'whole', 'system', 'public', 'education', 'around', 'world', 'protract', 'process', 'university', 'entrance', 'consequence', 'many', 'highly', 'talented', 'brilliant', 'creative', 'people', 'think', 'thing', 'good', 'school', 'value', 'actually', 'stigmatize', 'think', 'afford', 'go', 'way', 'next', 'years', 'accord', 'unesco', 'people', 'worldwide', 'graduate', 'education', 'since', 'beginning', 'history', 'people', 'combination', 'things', 'talk', 'technology', 'transformation', 'effect', 'work', 'demography', 'huge', 'explosion', 'population', 'suddenly', 'degree', 'worth', 'anything', 'true', 'student', 'degree', 'job', 'job', 'want', 'one', 'want', 'one', 'frankly', 'kid', 'degree', 'often', 'heading', 'home', 'carry', 'playing', 'video', 'game', 'need', 'previous', 'job', 'require', 'ba', 'need', 'phd', 'process', 'academic', 'inflation', 'indicate', 'whole', 'structure', 'education', 'shifting', 'beneath', 'foot', 'need', 'radically', 'rethink', 'view', 'intelligence', 'know', 'three', 'things', 'intelligence', 'one', 'diverse', 'think', 'world', 'ways', 'experience', 'think', 'visually', 'think', 'sound', 'think', 'kinesthetically', 'think', 'abstract', 'terms', 'think', 'movement', 'secondly', 'intelligence', 'dynamic', 'look', 'interaction', 'human', 'brain', 'hear', 'yesterday', 'number', 'presentation', 'intelligence', 'wonderfully', 'interactive', 'brain', 'divide', 'compartment', 'fact', 'creativity', 'define', 'process', 'original', 'idea', 'value', 'often', 'come', 'interaction', 'different', 'disciplinary', 'ways', 'seeing', 'things', 'way', 'shaft', 'nerves', 'join', 'two', 'half', 'brain', 'call', 'corpus', 'callosum', 'thick', 'woman', 'following', 'helen', 'yesterday', 'probably', 'woman', 'better', 'multi', 'task', 'raft', 'research', 'know', 'personal', 'life', 'wife', 'cooking', 'meal', 'home', 'often', 'thankfully', 'good', 'things', 'cooking', 'dealing', 'people', 'phone', 'talking', 'kid', 'painting', 'ceiling', 'open', 'heart', 'surgery', 'cooking', 'door', 'shut', 'kid', 'phone', 'hook', 'come', 'get', 'annoy', 'say', 'terry', 'please', 'try', 'fry', 'egg', 'give', 'break', 'actually', 'know', 'old', 'philosophical', 'thing', 'tree', 'falls', 'forest', 'nobody', 'hear', 'happen', 'remember', 'old', 'chestnut', 'saw', 'great', 'shirt', 'recently', 'say', 'man', 'speak', 'mind', 'forest', 'woman', 'hear', 'still', 'wrong', 'third', 'thing', 'intelligence', 'distinct', 'new', 'book', 'moment', 'call', 'epiphany', 'base', 'series', 'interview', 'people', 'discover', 'talent', 'fascinate', 'people', 'get', 'really', 'prompt', 'conversation', 'wonderful', 'woman', 'maybe', 'people', 'never', 'hear', 'gillian', 'lynne', 'hear', 'choreographer', 'everybody', 'know', 'work', 'cat', 'phantom', 'opera', 'wonderful', 'use', 'board', 'royal', 'ballet', 'see', 'anyway', 'gillian', 'lunch', 'one', 'day', 'say', 'get', 'dancer', 'interest', 'school', 'really', 'hopeless', 'school', 'write', 'parent', 'say', 'think', 'gillian', 'learning', 'disorder', 'concentrate', 'fidget', 'think', 'say', 'adhd', 'adhd', 'invent', 'point', 'available', 'condition', 'people', 'aware', 'could', 'anyway', 'go', 'see', 'specialist', 'oak', 'panel', 'room', 'mother', 'led', 'sat', 'chair', 'end', 'sat', 'hands', 'minutes', 'man', 'talk', 'mother', 'problem', 'gillian', 'school', 'disturb', 'people', 'homework', 'always', 'late', 'little', 'kid', 'eight', 'end', 'doctor', 'go', 'sat', 'next', 'gillian', 'say', 'listen', 'things', 'mother', 'tell', 'need', 'speak', 'privately', 'wait', 'back', 'long', 'go', 'left', 'go', 'room', 'turn', 'radio', 'sitting', 'desk', 'get', 'say', 'mother', 'stand', 'watch', 'minute', 'left', 'room', 'foot', 'move', 'music', 'watch', 'minutes', 'turn', 'mother', 'say', 'mrs', 'lynne', 'gillian', 'sick', 'dancer', 'take', 'dance', 'school', 'say', 'happen', 'say', 'tell', 'wonderful', 'walk', 'room', 'full', 'people', 'like', 'people', 'sit', 'still', 'people', 'move', 'think', 'move', 'think', 'ballet', 'tap', 'jazz', 'modern', 'contemporary', 'eventually', 'audition', 'royal', 'ballet', 'school', 'become', 'soloist', 'wonderful', 'career', 'royal', 'ballet', 'eventually', 'graduate', 'royal', 'ballet', 'school', 'found', 'gillian', 'lynne', 'dance', 'company', 'meet', 'andrew', 'lloyd', 'webber', 'responsible', 'successful', 'musical', 'theater', 'production', 'history', 'given', 'pleasure', 'million', 'multi', 'millionaire', 'somebody', 'else', 'might', 'put', 'medication', 'tell', 'calm', 'think', 'come', 'al', 'gore', 'spoke', 'night', 'ecology', 'revolution', 'trigger', 'rachel', 'carson', 'believe', 'hope', 'future', 'adopt', 'new', 'conception', 'human', 'ecology', 'one', 'start', 'reconstitute', 'conception', 'richness', 'human', 'capacity', 'education', 'system', 'mine', 'mind', 'way', 'strip', 'mine', 'earth', 'particular', 'commodity', 'future', 'serve', 'us', 'rethink', 'fundamental', 'principle', 'educate', 'child', 'wonderful', 'quote', 'jonas', 'salk', 'say', 'insect', 'disappear', 'earth', 'within', 'years', 'life', 'earth', 'would', 'end', 'human', 'being', 'disappear', 'earth', 'within', 'years', 'form', 'life', 'would', 'flourish', 'right', 'ted', 'celebrate', 'gift', 'human', 'imagination', 'careful', 'use', 'gift', 'wisely', 'avert', 'scenario', 'talk', 'way', 'seeing', 'creative', 'capacity', 'richness', 'seeing', 'child', 'hope', 'task', 'educate', 'whole', 'face', 'future', 'way', 'may', 'see', 'future', 'job', 'help', 'make', 'something', 'thank', 'much']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data1['transcript']))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 4), (3, 1), (4, 1), (5, 6), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 3), (24, 1), (25, 1), (26, 1), (27, 2), (28, 2), (29, 3), (30, 3), (31, 2), (32, 3), (33, 2), (34, 3), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 3), (44, 5), (45, 1), (46, 1), (47, 1), (48, 2), (49, 2), (50, 1), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 1), (57, 3), (58, 1), (59, 1), (60, 1), (61, 1), (62, 3), (63, 1), (64, 3), (65, 1), (66, 1), (67, 5), (68, 3), (69, 1), (70, 1), (71, 2), (72, 1), (73, 3), (74, 1), (75, 1), (76, 6), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 9), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 12), (95, 1), (96, 1), (97, 2), (98, 1), (99, 1), (100, 2), (101, 2), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (107, 1), (108, 2), (109, 1), (110, 3), (111, 1), (112, 2), (113, 1), (114, 1), (115, 4), (116, 5), (117, 2), (118, 1), (119, 1), (120, 6), (121, 2), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 3), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 2), (136, 2), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 5), (151, 1), (152, 6), (153, 2), (154, 6), (155, 22), (156, 1), (157, 1), (158, 1), (159, 1), (160, 4), (161, 2), (162, 1), (163, 1), (164, 1), (165, 1), (166, 2), (167, 1), (168, 3), (169, 3), (170, 1), (171, 1), (172, 2), (173, 2), (174, 2), (175, 3), (176, 1), (177, 1), (178, 4), (179, 2), (180, 3), (181, 1), (182, 2), (183, 2), (184, 1), (185, 1), (186, 3), (187, 1), (188, 2), (189, 1), (190, 1), (191, 1), (192, 2), (193, 2), (194, 4), (195, 2), (196, 5), (197, 1), (198, 1), (199, 1), (200, 3), (201, 2), (202, 1), (203, 2), (204, 1), (205, 6), (206, 1), (207, 11), (208, 1), (209, 1), (210, 3), (211, 7), (212, 5), (213, 1), (214, 1), (215, 2), (216, 13), (217, 3), (218, 2), (219, 2), (220, 3), (221, 1), (222, 2), (223, 1), (224, 3), (225, 1), (226, 5), (227, 1), (228, 1), (229, 5), (230, 1), (231, 1), (232, 5), (233, 1), (234, 6), (235, 1), (236, 1), (237, 1), (238, 3), (239, 1), (240, 1), (241, 1), (242, 2), (243, 1), (244, 3), (245, 1), (246, 1), (247, 1), (248, 2), (249, 1), (250, 2), (251, 7), (252, 1), (253, 5), (254, 1), (255, 1), (256, 1), (257, 1), (258, 3), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 6), (265, 2), (266, 1), (267, 6), (268, 1), (269, 1), (270, 2), (271, 1), (272, 5), (273, 1), (274, 1), (275, 2), (276, 9), (277, 1), (278, 1), (279, 1), (280, 14), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (286, 2), (287, 1), (288, 3), (289, 2), (290, 6), (291, 10), (292, 1), (293, 1), (294, 1), (295, 3), (296, 4), (297, 1), (298, 2), (299, 5), (300, 3), (301, 1), (302, 3), (303, 1), (304, 3), (305, 1), (306, 2), (307, 2), (308, 1), (309, 1), (310, 1), (311, 1), (312, 2), (313, 1), (314, 3), (315, 1), (316, 1), (317, 5), (318, 1), (319, 2), (320, 2), (321, 1), (322, 1), (323, 1), (324, 1), (325, 1), (326, 3), (327, 2), (328, 2), (329, 3), (330, 1), (331, 3), (332, 1), (333, 1), (334, 1), (335, 1), (336, 1), (337, 5), (338, 6), (339, 1), (340, 1), (341, 3), (342, 2), (343, 3), (344, 1), (345, 1), (346, 1), (347, 2), (348, 1), (349, 2), (350, 5), (351, 1), (352, 6), (353, 3), (354, 2), (355, 4), (356, 3), (357, 1), (358, 1), (359, 2), (360, 1), (361, 4), (362, 1), (363, 1), (364, 3), (365, 14), (366, 1), (367, 1), (368, 2), (369, 1), (370, 1), (371, 1), (372, 1), (373, 1), (374, 1), (375, 1), (376, 1), (377, 2), (378, 1), (379, 1), (380, 1), (381, 2), (382, 1), (383, 1), (384, 1), (385, 1), (386, 16), (387, 1), (388, 1), (389, 1), (390, 1), (391, 1), (392, 2), (393, 1), (394, 1), (395, 1), (396, 3), (397, 1), (398, 1), (399, 2), (400, 1), (401, 2), (402, 1), (403, 3), (404, 1), (405, 1), (406, 1), (407, 2), (408, 2), (409, 2), (410, 1), (411, 1), (412, 1), (413, 2), (414, 2), (415, 3), (416, 1), (417, 1), (418, 3), (419, 1), (420, 1), (421, 1), (422, 1), (423, 4), (424, 1), (425, 4), (426, 1), (427, 1), (428, 1), (429, 1), (430, 1), (431, 1), (432, 4), (433, 1), (434, 8), (435, 2), (436, 2), (437, 1), (438, 1), (439, 1), (440, 1), (441, 2), (442, 1), (443, 1), (444, 1), (445, 1), (446, 1), (447, 2), (448, 1), (449, 2), (450, 2), (451, 4), (452, 1), (453, 4), (454, 1), (455, 4), (456, 2), (457, 2), (458, 1), (459, 1), (460, 2), (461, 3), (462, 1), (463, 32), (464, 1), (465, 10), (466, 1), (467, 3), (468, 1), (469, 6), (470, 4), (471, 1), (472, 2), (473, 1), (474, 1), (475, 1), (476, 1), (477, 2), (478, 1), (479, 5), (480, 1), (481, 2), (482, 1), (483, 1), (484, 2), (485, 1), (486, 1), (487, 1), (488, 2), (489, 1), (490, 2), (491, 1), (492, 1), (493, 3), (494, 3), (495, 2), (496, 1), (497, 4), (498, 1), (499, 1), (500, 1), (501, 1), (502, 1), (503, 2), (504, 1), (505, 2), (506, 1), (507, 2), (508, 2), (509, 1), (510, 3), (511, 1), (512, 4), (513, 1), (514, 2), (515, 1), (516, 1), (517, 1), (518, 2), (519, 1), (520, 1), (521, 1), (522, 1), (523, 1), (524, 9), (525, 3), (526, 3), (527, 1), (528, 7), (529, 1), (530, 1), (531, 2), (532, 1), (533, 2), (534, 3), (535, 1), (536, 1), (537, 3), (538, 1), (539, 2), (540, 1), (541, 1), (542, 3), (543, 1), (544, 1), (545, 1), (546, 1), (547, 6), (548, 10), (549, 26), (550, 3), (551, 2), (552, 4), (553, 1), (554, 1), (555, 4), (556, 3), (557, 1), (558, 1), (559, 2), (560, 1), (561, 1), (562, 1), (563, 1), (564, 1), (565, 1), (566, 1), (567, 1), (568, 2), (569, 2), (570, 3), (571, 1), (572, 1), (573, 1), (574, 4), (575, 1), (576, 1), (577, 3), (578, 3), (579, 1), (580, 2), (581, 1), (582, 1), (583, 1), (584, 2), (585, 1), (586, 1), (587, 1), (588, 1), (589, 1), (590, 1), (591, 1), (592, 9), (593, 2), (594, 1), (595, 9), (596, 2), (597, 1), (598, 1), (599, 1), (600, 1), (601, 7), (602, 1), (603, 1), (604, 1), (605, 3), (606, 5), (607, 5), (608, 1), (609, 1), (610, 6), (611, 7), (612, 1), (613, 1), (614, 1), (615, 5), (616, 2), (617, 1), (618, 7), (619, 1), (620, 3), (621, 5), (622, 2), (623, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ability',\n",
       " 'abstract',\n",
       " 'academic',\n",
       " 'accord',\n",
       " 'achievement',\n",
       " 'actually',\n",
       " 'adhd',\n",
       " 'adopt',\n",
       " 'adult',\n",
       " 'advice',\n",
       " 'affection',\n",
       " 'afford',\n",
       " 'afterward',\n",
       " 'agent',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'al',\n",
       " 'alien',\n",
       " 'allow',\n",
       " 'along',\n",
       " 'always',\n",
       " 'america',\n",
       " 'andrew',\n",
       " 'angeles',\n",
       " 'anniversary',\n",
       " 'annoy',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'around',\n",
       " 'art',\n",
       " 'artist',\n",
       " 'arts',\n",
       " 'ask',\n",
       " 'attention',\n",
       " 'audition',\n",
       " 'available',\n",
       " 'avert',\n",
       " 'avon',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'ba',\n",
       " 'back',\n",
       " 'ballet',\n",
       " 'base',\n",
       " 'bearing',\n",
       " 'beat',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'beginning',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'beneath',\n",
       " 'benign',\n",
       " 'benignly',\n",
       " 'better',\n",
       " 'big',\n",
       " 'bit',\n",
       " 'blood',\n",
       " 'blow',\n",
       " 'board',\n",
       " 'body',\n",
       " 'book',\n",
       " 'born',\n",
       " 'bottom',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'brain',\n",
       " 'break',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'brownie',\n",
       " 'call',\n",
       " 'callosum',\n",
       " 'calm',\n",
       " 'capacity',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'carry',\n",
       " 'carson',\n",
       " 'cat',\n",
       " 'ceiling',\n",
       " 'celebrate',\n",
       " 'century',\n",
       " 'chair',\n",
       " 'chance',\n",
       " 'chestnut',\n",
       " 'child',\n",
       " 'childhood',\n",
       " 'choreographer',\n",
       " 'class',\n",
       " 'clue',\n",
       " 'combination',\n",
       " 'come',\n",
       " 'commodity',\n",
       " 'common',\n",
       " 'company',\n",
       " 'compartment',\n",
       " 'concentrate',\n",
       " 'conception',\n",
       " 'conclude',\n",
       " 'condition',\n",
       " 'conference',\n",
       " 'confuse',\n",
       " 'consequence',\n",
       " 'consider',\n",
       " 'contemporary',\n",
       " 'contention',\n",
       " 'conversation',\n",
       " 'cooking',\n",
       " 'corpus',\n",
       " 'could',\n",
       " 'country',\n",
       " 'cram',\n",
       " 'creative',\n",
       " 'creativity',\n",
       " 'curious',\n",
       " 'curiously',\n",
       " 'dad',\n",
       " 'dance',\n",
       " 'dancer',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dealing',\n",
       " 'dedication',\n",
       " 'deep',\n",
       " 'define',\n",
       " 'degree',\n",
       " 'demography',\n",
       " 'design',\n",
       " 'desk',\n",
       " 'despite',\n",
       " 'different',\n",
       " 'dinner',\n",
       " 'disappear',\n",
       " 'disciplinary',\n",
       " 'discotheque',\n",
       " 'discover',\n",
       " 'disembody',\n",
       " 'disorder',\n",
       " 'distinct',\n",
       " 'disturb',\n",
       " 'diverse',\n",
       " 'divide',\n",
       " 'doctor',\n",
       " 'dominate',\n",
       " 'door',\n",
       " 'drama',\n",
       " 'drawing',\n",
       " 'dynamic',\n",
       " 'earth',\n",
       " 'ecology',\n",
       " 'educate',\n",
       " 'education',\n",
       " 'effect',\n",
       " 'egg',\n",
       " 'eight',\n",
       " 'else',\n",
       " 'end',\n",
       " 'england',\n",
       " 'english',\n",
       " 'engulf',\n",
       " 'entrance',\n",
       " 'epiphany',\n",
       " 'eventually',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyday',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'evidence',\n",
       " 'exceptional',\n",
       " 'experience',\n",
       " 'expertise',\n",
       " 'explosion',\n",
       " 'extraordinary',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'falls',\n",
       " 'fascinate',\n",
       " 'father',\n",
       " 'fidget',\n",
       " 'final',\n",
       " 'find',\n",
       " 'first',\n",
       " 'five',\n",
       " 'flourish',\n",
       " 'focus',\n",
       " 'following',\n",
       " 'foot',\n",
       " 'forest',\n",
       " 'form',\n",
       " 'found',\n",
       " 'four',\n",
       " 'fourth',\n",
       " 'frank',\n",
       " 'frankincense',\n",
       " 'frankly',\n",
       " 'frighten',\n",
       " 'fry',\n",
       " 'full',\n",
       " 'fundamental',\n",
       " 'future',\n",
       " 'game',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'gibson',\n",
       " 'gift',\n",
       " 'gillian',\n",
       " 'girl',\n",
       " 'girlfriend',\n",
       " 'give',\n",
       " 'given',\n",
       " 'go',\n",
       " 'god',\n",
       " 'going',\n",
       " 'gold',\n",
       " 'good',\n",
       " 'gore',\n",
       " 'graduate',\n",
       " 'grasp',\n",
       " 'great',\n",
       " 'grounds',\n",
       " 'grow',\n",
       " 'half',\n",
       " 'hands',\n",
       " 'happen',\n",
       " 'hard',\n",
       " 'hardly',\n",
       " 'head',\n",
       " 'heading',\n",
       " 'hear',\n",
       " 'heart',\n",
       " 'helen',\n",
       " 'help',\n",
       " 'hierarchy',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highly',\n",
       " 'history',\n",
       " 'hold',\n",
       " 'home',\n",
       " 'homework',\n",
       " 'honest',\n",
       " 'hook',\n",
       " 'hope',\n",
       " 'hopeless',\n",
       " 'huge',\n",
       " 'human',\n",
       " 'humanities',\n",
       " 'idea',\n",
       " 'ii',\n",
       " 'image',\n",
       " 'imagination',\n",
       " 'imagine',\n",
       " 'important',\n",
       " 'indicate',\n",
       " 'industrialism',\n",
       " 'inflation',\n",
       " 'innovation',\n",
       " 'insect',\n",
       " 'intelligence',\n",
       " 'interaction',\n",
       " 'interactive',\n",
       " 'interest',\n",
       " 'interview',\n",
       " 'invent',\n",
       " 'james',\n",
       " 'jazz',\n",
       " 'job',\n",
       " 'join',\n",
       " 'jonas',\n",
       " 'joseph',\n",
       " 'kid',\n",
       " 'kind',\n",
       " 'kinesthetically',\n",
       " 'king',\n",
       " 'know',\n",
       " 'language',\n",
       " 'last',\n",
       " 'late',\n",
       " 'lead',\n",
       " 'learning',\n",
       " 'leaving',\n",
       " 'led',\n",
       " 'left',\n",
       " 'lesson',\n",
       " 'life',\n",
       " 'like',\n",
       " 'listen',\n",
       " 'literacy',\n",
       " 'literal',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lloyd',\n",
       " 'long',\n",
       " 'look',\n",
       " 'los',\n",
       " 'lost',\n",
       " 'love',\n",
       " 'lunch',\n",
       " 'lynne',\n",
       " 'main',\n",
       " 'make',\n",
       " 'man',\n",
       " 'many',\n",
       " 'mark',\n",
       " 'marvel',\n",
       " 'math',\n",
       " 'mathematics',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'meal',\n",
       " 'mean',\n",
       " 'medication',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'mel',\n",
       " 'men',\n",
       " 'might',\n",
       " 'million',\n",
       " 'millionaire',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'mistake',\n",
       " 'modern',\n",
       " 'moment',\n",
       " 'money',\n",
       " 'month',\n",
       " 'morning',\n",
       " 'mother',\n",
       " 'move',\n",
       " 'movement',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'multi',\n",
       " 'music',\n",
       " 'musical',\n",
       " 'musician',\n",
       " 'must',\n",
       " 'myrrh',\n",
       " 'national',\n",
       " 'nativity',\n",
       " 'need',\n",
       " 'nerves',\n",
       " 'never',\n",
       " 'new',\n",
       " 'next',\n",
       " 'night',\n",
       " 'nobody',\n",
       " 'nonetheless',\n",
       " 'normally',\n",
       " 'number',\n",
       " 'oak',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'old',\n",
       " 'one',\n",
       " 'open',\n",
       " 'opera',\n",
       " 'original',\n",
       " 'otherwise',\n",
       " 'output',\n",
       " 'outside',\n",
       " 'painting',\n",
       " 'panel',\n",
       " 'paper',\n",
       " 'parade',\n",
       " 'parent',\n",
       " 'part',\n",
       " 'particular',\n",
       " 'partly',\n",
       " 'parts',\n",
       " 'party',\n",
       " 'passionately',\n",
       " 'past',\n",
       " 'pay',\n",
       " 'pencil',\n",
       " 'people',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'phantom',\n",
       " 'phd',\n",
       " 'philosophical',\n",
       " 'phone',\n",
       " 'picasso',\n",
       " 'picture',\n",
       " 'pin',\n",
       " 'place',\n",
       " 'plane',\n",
       " 'planet',\n",
       " 'play',\n",
       " 'playing',\n",
       " 'please',\n",
       " 'pleasure',\n",
       " 'point',\n",
       " 'pop',\n",
       " 'population',\n",
       " 'predicate',\n",
       " 'prepare',\n",
       " 'presentation',\n",
       " 'pretty',\n",
       " 'previous',\n",
       " 'principle',\n",
       " 'privately',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'process',\n",
       " 'produce',\n",
       " 'production',\n",
       " 'professor',\n",
       " 'profoundly',\n",
       " 'progressively',\n",
       " 'prompt',\n",
       " 'protract',\n",
       " 'public',\n",
       " 'purpose',\n",
       " 'put',\n",
       " 'quote',\n",
       " 'rachel',\n",
       " 'radically',\n",
       " 'radio',\n",
       " 'raft',\n",
       " 'range',\n",
       " 'rather',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'recently',\n",
       " 'reconstitute',\n",
       " 'relevant',\n",
       " 'religion',\n",
       " 'remain',\n",
       " 'remember',\n",
       " 'require',\n",
       " 'research',\n",
       " 'residential',\n",
       " 'responsible',\n",
       " 'result',\n",
       " 'rethink',\n",
       " 'retire',\n",
       " 'revolution',\n",
       " 'richness',\n",
       " 'right',\n",
       " 'robinson',\n",
       " 'room',\n",
       " 'root',\n",
       " 'royal',\n",
       " 'run',\n",
       " 'running',\n",
       " 'ruthlessly',\n",
       " 'salk',\n",
       " 'sarah',\n",
       " 'sat',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'scenario',\n",
       " 'school',\n",
       " 'seamless',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'senior',\n",
       " 'sent',\n",
       " 'sequel',\n",
       " 'sequence',\n",
       " 'series',\n",
       " 'serve',\n",
       " 'seven',\n",
       " 'shaft',\n",
       " 'shakespeare',\n",
       " 'shifting',\n",
       " 'shirt',\n",
       " 'shut',\n",
       " 'sick',\n",
       " 'side',\n",
       " 'since',\n",
       " 'sirena',\n",
       " 'sit',\n",
       " 'sitting',\n",
       " 'six',\n",
       " 'slightly',\n",
       " 'snitterfield',\n",
       " 'soloist',\n",
       " 'somebody',\n",
       " 'something',\n",
       " 'son',\n",
       " 'sound',\n",
       " 'speak',\n",
       " 'speaking',\n",
       " 'specialist',\n",
       " 'spoke',\n",
       " 'squander',\n",
       " 'stand',\n",
       " 'start',\n",
       " 'starting',\n",
       " 'status',\n",
       " 'steer',\n",
       " 'stigmatize',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'story',\n",
       " 'strange',\n",
       " 'stratford',\n",
       " 'strict',\n",
       " 'strike',\n",
       " 'strip',\n",
       " 'structure',\n",
       " 'student',\n",
       " 'subject',\n",
       " 'succeed',\n",
       " 'successful',\n",
       " 'suddenly',\n",
       " 'surgery',\n",
       " 'switch',\n",
       " 'system',\n",
       " 'take',\n",
       " 'talent',\n",
       " 'talented',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tap',\n",
       " 'task',\n",
       " 'tea',\n",
       " 'teach',\n",
       " 'teacher',\n",
       " 'technology',\n",
       " 'ted',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'terms',\n",
       " 'terry',\n",
       " 'th',\n",
       " 'thank',\n",
       " 'thankfully',\n",
       " 'theater',\n",
       " 'theme',\n",
       " 'thick',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'third',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'thrill',\n",
       " 'throughout',\n",
       " 'time',\n",
       " 'top',\n",
       " 'towel',\n",
       " 'transformation',\n",
       " 'transition',\n",
       " 'transport',\n",
       " 'travel',\n",
       " 'treat',\n",
       " 'tree',\n",
       " 'tremendous',\n",
       " 'trigger',\n",
       " 'true',\n",
       " 'truthfully',\n",
       " 'try',\n",
       " 'turn',\n",
       " 'two',\n",
       " 'typically',\n",
       " 'uncontrollably',\n",
       " 'unesco',\n",
       " 'university',\n",
       " 'upon',\n",
       " 'upset',\n",
       " 'us',\n",
       " 'use',\n",
       " 'useful',\n",
       " 'value',\n",
       " 'variety',\n",
       " 'vest',\n",
       " 'video',\n",
       " 'view',\n",
       " 'visit',\n",
       " 'visually',\n",
       " 'waist',\n",
       " 'wait',\n",
       " 'waiting',\n",
       " 'walk',\n",
       " 'wall',\n",
       " 'want',\n",
       " 'watch',\n",
       " 'water',\n",
       " 'way',\n",
       " 'ways',\n",
       " 'webber',\n",
       " 'week',\n",
       " 'well',\n",
       " 'wherever',\n",
       " 'whole',\n",
       " 'wife',\n",
       " 'winner',\n",
       " 'wisely',\n",
       " 'within',\n",
       " 'woman',\n",
       " 'wonderful',\n",
       " 'wonderfully',\n",
       " 'word',\n",
       " 'work',\n",
       " 'world',\n",
       " 'worldwide',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'write',\n",
       " 'writhe',\n",
       " 'wrong',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yesterday',\n",
       " 'yet',\n",
       " 'accomplish',\n",
       " 'acquire',\n",
       " 'across',\n",
       " 'active',\n",
       " 'ad',\n",
       " 'add',\n",
       " 'address',\n",
       " 'africa',\n",
       " 'air',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'already',\n",
       " 'also',\n",
       " 'american',\n",
       " 'anderson',\n",
       " 'announce',\n",
       " 'annual',\n",
       " 'anybody',\n",
       " 'app',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'arcane',\n",
       " 'architecture',\n",
       " 'arm',\n",
       " 'asleep',\n",
       " 'atlantic',\n",
       " 'automobile',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'azores',\n",
       " 'bad',\n",
       " 'basis',\n",
       " 'beach',\n",
       " 'begin',\n",
       " 'bill',\n",
       " 'bipartisan',\n",
       " 'boot',\n",
       " 'booth',\n",
       " 'brand',\n",
       " 'branding',\n",
       " 'building',\n",
       " 'bunch',\n",
       " 'burger',\n",
       " 'business',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'calculate',\n",
       " 'calculation',\n",
       " 'calculator',\n",
       " 'campaign',\n",
       " 'cap',\n",
       " 'capital',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'carbon',\n",
       " 'catalyst',\n",
       " 'cc',\n",
       " 'ceo',\n",
       " 'chain',\n",
       " 'change',\n",
       " 'check',\n",
       " 'chef',\n",
       " 'choice',\n",
       " 'chris',\n",
       " 'city',\n",
       " 'click',\n",
       " 'climate',\n",
       " 'climatecrisis',\n",
       " 'clinton',\n",
       " 'close',\n",
       " 'co',\n",
       " 'coherence',\n",
       " 'collapse',\n",
       " 'colleague',\n",
       " 'combing',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'commotion',\n",
       " 'community',\n",
       " 'compensate',\n",
       " 'complain',\n",
       " 'completely',\n",
       " 'concern',\n",
       " 'conducting',\n",
       " 'congratulations',\n",
       " 'connect',\n",
       " 'conservation',\n",
       " 'construct',\n",
       " 'consumer',\n",
       " 'continue',\n",
       " 'contribution',\n",
       " 'convene',\n",
       " 'convince',\n",
       " 'copyright',\n",
       " 'corporation',\n",
       " 'cost',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'crisis',\n",
       " 'current',\n",
       " 'cutting',\n",
       " 'david',\n",
       " 'deal',\n",
       " 'decision',\n",
       " 'deflect',\n",
       " 'democracy',\n",
       " 'democrat',\n",
       " 'difference',\n",
       " 'dinnertime',\n",
       " 'dioxide',\n",
       " 'directly',\n",
       " 'director',\n",
       " 'do',\n",
       " 'doerr',\n",
       " 'driving',\n",
       " 'east',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'edge',\n",
       " 'effective',\n",
       " 'efficiency',\n",
       " 'elaborate',\n",
       " 'electricity',\n",
       " 'emission',\n",
       " 'en',\n",
       " 'enable',\n",
       " 'energy',\n",
       " 'ensure',\n",
       " 'entertain',\n",
       " 'entertainment',\n",
       " 'environment',\n",
       " 'except',\n",
       " 'exit',\n",
       " 'expenditure',\n",
       " 'extremely',\n",
       " 'facing',\n",
       " 'family',\n",
       " 'farm',\n",
       " 'fell',\n",
       " 'figure',\n",
       " 'fly',\n",
       " 'folks',\n",
       " 'force',\n",
       " 'ford',\n",
       " 'former',\n",
       " 'fossil',\n",
       " 'fresh',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'fries',\n",
       " 'fruit',\n",
       " 'fuel',\n",
       " 'generic',\n",
       " 'global',\n",
       " 'grateful',\n",
       " 'green',\n",
       " 'group',\n",
       " 'habit',\n",
       " 'handwritten',\n",
       " 'hanging',\n",
       " 'harsh',\n",
       " 'hat',\n",
       " 'historical',\n",
       " 'hit',\n",
       " 'honor',\n",
       " 'hot',\n",
       " 'house',\n",
       " 'hybrid',\n",
       " 'illustrate',\n",
       " 'include',\n",
       " 'income',\n",
       " 'influence',\n",
       " 'information',\n",
       " 'instead',\n",
       " 'insulation',\n",
       " 'integrate',\n",
       " 'invest',\n",
       " 'investment',\n",
       " 'involve',\n",
       " 'involvement',\n",
       " 'island',\n",
       " 'issue',\n",
       " 'january',\n",
       " 'jay',\n",
       " 'john',\n",
       " 'judge',\n",
       " 'keep',\n",
       " 'killer',\n",
       " 'lagos',\n",
       " 'land',\n",
       " 'larry',\n",
       " 'later',\n",
       " 'laugh',\n",
       " 'le',\n",
       " 'leading',\n",
       " 'learn',\n",
       " 'lebanon',\n",
       " 'legal',\n",
       " 'length',\n",
       " 'leno',\n",
       " 'lessig',\n",
       " 'let',\n",
       " 'letter',\n",
       " 'letterman',\n",
       " 'level',\n",
       " 'liability',\n",
       " 'lifelong',\n",
       " 'light',\n",
       " 'likely',\n",
       " 'limb',\n",
       " 'limited',\n",
       " 'link',\n",
       " 'logic',\n",
       " 'longer',\n",
       " 'looking',\n",
       " 'lot',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'lowest',\n",
       " 'majora',\n",
       " 'making',\n",
       " 'management',\n",
       " 'manager',\n",
       " 'market',\n",
       " 'mass',\n",
       " 'masse',\n",
       " 'material',\n",
       " 'maximum',\n",
       " 'means',\n",
       " 'mediate',\n",
       " 'mention',\n",
       " 'middle',\n",
       " 'mile',\n",
       " 'mirror',\n",
       " 'monterey',\n",
       " 'motorcade',\n",
       " 'movie',\n",
       " 'name',\n",
       " 'nashville',\n",
       " 'negative',\n",
       " 'neocortex',\n",
       " 'net',\n",
       " 'neutral',\n",
       " 'news',\n",
       " 'nice',\n",
       " 'nigeria',\n",
       " 'nominate',\n",
       " 'obviously',\n",
       " 'offset',\n",
       " 'opportunity',\n",
       " 'opposable',\n",
       " 'option',\n",
       " 'order',\n",
       " 'others',\n",
       " 'ought',\n",
       " 'pain',\n",
       " 'participant',\n",
       " 'participation',\n",
       " 'partisan',\n",
       " 'partner',\n",
       " 'path',\n",
       " 'performance',\n",
       " 'permission',\n",
       " 'personally',\n",
       " 'perspective',\n",
       " 'persuasion',\n",
       " 'pick',\n",
       " 'piece',\n",
       " 'plan',\n",
       " 'political',\n",
       " 'politically',\n",
       " 'politician',\n",
       " 'politics',\n",
       " 'pollution',\n",
       " 'position',\n",
       " 'positive',\n",
       " 'post',\n",
       " 'power',\n",
       " 'precisely',\n",
       " 'presently',\n",
       " 'president',\n",
       " 'print',\n",
       " 'profit',\n",
       " 'profitable',\n",
       " 'project',\n",
       " 'projection',\n",
       " 'purchase',\n",
       " 'puzzle',\n",
       " 'quarterly',\n",
       " 'quick',\n",
       " 'quite',\n",
       " 'rail',\n",
       " 'rapidly',\n",
       " 'rear',\n",
       " 'recapitulate',\n",
       " 'recapitulation',\n",
       " 'record',\n",
       " 'reduce',\n",
       " 'reducing',\n",
       " 'refueling',\n",
       " 'remainder',\n",
       " 'remix',\n",
       " 'renewables',\n",
       " 'rent',\n",
       " 'repeat',\n",
       " 'repetition',\n",
       " 'replace',\n",
       " 'report',\n",
       " 'republican',\n",
       " 'restaurant',\n",
       " 'return',\n",
       " 'role',\n",
       " 'runway',\n",
       " 'safe',\n",
       " 'saying',\n",
       " 'science',\n",
       " 'scientist',\n",
       " 'sending',\n",
       " 'sequestration',\n",
       " 'services',\n",
       " 'share',\n",
       " 'shell',\n",
       " 'shoes',\n",
       " 'shoney',\n",
       " 'short',\n",
       " 'show',\n",
       " 'sign',\n",
       " 'significant',\n",
       " 'sincerely',\n",
       " 'single',\n",
       " 'slide',\n",
       " 'slideshow',\n",
       " 'software',\n",
       " 'soil',\n",
       " 'solution',\n",
       " 'solve',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'speech',\n",
       " 'spring',\n",
       " 'staff',\n",
       " 'stage',\n",
       " 'standard',\n",
       " 'state',\n",
       " 'stay',\n",
       " 'strain',\n",
       " 'success',\n",
       " 'sudden',\n",
       " 'suggest',\n",
       " 'summer',\n",
       " 'support',\n",
       " 'suppose',\n",
       " 'sustainably',\n",
       " 'target',\n",
       " 'taurus',\n",
       " 'technological',\n",
       " 'television',\n",
       " 'temperature',\n",
       " 'tennessee',\n",
       " 'term',\n",
       " 'test',\n",
       " 'thinking',\n",
       " 'thumb',\n",
       " 'tide',\n",
       " 'tipper',\n",
       " 'together',\n",
       " 'tool',\n",
       " 'topic',\n",
       " 'totally',\n",
       " 'trading',\n",
       " 'train',\n",
       " 'transportation',\n",
       " 'truck',\n",
       " 'truly',\n",
       " 'twice',\n",
       " 'unite',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.expanduser('~/work/data/vocabulary.pkl'), 'wb') as output:\n",
    "    pickle.dump(np.array(list(id2word.values()), dtype=str), output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(27,\n",
      "  [('universe', 0.022666898924356575),\n",
      "   ('light', 0.016918521099159516),\n",
      "   ('theory', 0.011866916343683316),\n",
      "   ('space', 0.011692723076253103),\n",
      "   ('science', 0.01077820842224448),\n",
      "   ('number', 0.009036275747942342),\n",
      "   ('physics', 0.008796760005225798),\n",
      "   ('galaxy', 0.008753211688368245),\n",
      "   ('particle', 0.008687889213081914),\n",
      "   ('star', 0.008513695945651701)]),\n",
      " (14,\n",
      "  [('war', 0.0310866761691441),\n",
      "   ('people', 0.012421095499898187),\n",
      "   ('conflict', 0.009819226679336636),\n",
      "   ('kill', 0.009819226679336636),\n",
      "   ('military', 0.009230978076427069),\n",
      "   ('refugee', 0.008348605172062717),\n",
      "   ('world', 0.007828231407950406),\n",
      "   ('peace', 0.0076924817303558905),\n",
      "   ('violence', 0.007488857213964117),\n",
      "   ('force', 0.007330482590103849)]),\n",
      " (24,\n",
      "  [('city', 0.06589404731911774),\n",
      "   ('place', 0.02272050571448203),\n",
      "   ('people', 0.021048533015734866),\n",
      "   ('building', 0.016857149401067313),\n",
      "   ('street', 0.015574540207507844),\n",
      "   ('space', 0.013994182451157784),\n",
      "   ('york', 0.011245734179244635),\n",
      "   ('build', 0.010489910904468519),\n",
      "   ('community', 0.010283777284075033),\n",
      "   ('map', 0.009963124985685166)]),\n",
      " (17,\n",
      "  [('gene', 0.01772740482417902),\n",
      "   ('dna', 0.017012049270113787),\n",
      "   ('life', 0.015849596494757784),\n",
      "   ('cell', 0.015134240940692554),\n",
      "   ('make', 0.014240046498111014),\n",
      "   ('years', 0.010685623588849395),\n",
      "   ('molecule', 0.00927726734178347),\n",
      "   ('biology', 0.009187847897525317),\n",
      "   ('bacteria', 0.009031363870073547),\n",
      "   ('organism', 0.009009009009009009)]),\n",
      " (32,\n",
      "  [('child', 0.050769287532745455),\n",
      "   ('family', 0.0380031730804708),\n",
      "   ('life', 0.03719145482049958),\n",
      "   ('mother', 0.02488654392502675),\n",
      "   ('years', 0.02270966313692211),\n",
      "   ('father', 0.01739659816256503),\n",
      "   ('baby', 0.016972290890307346),\n",
      "   ('home', 0.015810057927166735),\n",
      "   ('parent', 0.014186621407224292),\n",
      "   ('time', 0.013319558720436851)]),\n",
      " (5,\n",
      "  [('ocean', 0.025614077966515946),\n",
      "   ('fish', 0.016497617010876205),\n",
      "   ('sea', 0.01615544421361359),\n",
      "   ('water', 0.015202248564096297),\n",
      "   ('ice', 0.010142979347427594),\n",
      "   ('back', 0.009214224612000489),\n",
      "   ('coral', 0.007918856165220579),\n",
      "   ('place', 0.007478919711597214),\n",
      "   ('time', 0.00743003788341684),\n",
      "   ('shark', 0.007381156055236466)]),\n",
      " (2,\n",
      "  [('company', 0.028657564626793263),\n",
      "   ('dollar', 0.0279298634694019),\n",
      "   ('money', 0.027600665326772474),\n",
      "   ('business', 0.024672534479173888),\n",
      "   ('pay', 0.015091135906854252),\n",
      "   ('market', 0.015073809688821124),\n",
      "   ('work', 0.013081294615011436),\n",
      "   ('make', 0.01190311178875875),\n",
      "   ('buy', 0.011539261210063068),\n",
      "   ('job', 0.011227389285466769)]),\n",
      " (25,\n",
      "  [('life', 0.03479230755392448),\n",
      "   ('story', 0.03243564142695234),\n",
      "   ('world', 0.02775828880853437),\n",
      "   ('experience', 0.018835339198013924),\n",
      "   ('god', 0.015399284005253026),\n",
      "   ('feel', 0.011117707377624264),\n",
      "   ('moment', 0.010703941569071906),\n",
      "   ('live', 0.0102721859427564),\n",
      "   ('time', 0.009984348858546063),\n",
      "   ('mind', 0.009570583049993704)]),\n",
      " (9,\n",
      "  [('brain', 0.07124132613723978),\n",
      "   ('memory', 0.011278775195506113),\n",
      "   ('mind', 0.009890957153871572),\n",
      "   ('neuron', 0.009758784007049234),\n",
      "   ('control', 0.008613283401255644),\n",
      "   ('body', 0.00826082167639608),\n",
      "   ('change', 0.007159378786209935),\n",
      "   ('consciousness', 0.0068730036347615375),\n",
      "   ('show', 0.006696772772331754),\n",
      "   ('pattern', 0.006190109042846129)]),\n",
      " (26,\n",
      "  [('animal', 0.020014260775491136),\n",
      "   ('tree', 0.01981756042388926),\n",
      "   ('forest', 0.015367214968896757),\n",
      "   ('plant', 0.014334538122986894),\n",
      "   ('species', 0.013818199700031964),\n",
      "   ('world', 0.010351356003048855),\n",
      "   ('bird', 0.008556465294681714),\n",
      "   ('nature', 0.008138477047527723),\n",
      "   ('bee', 0.00796636423987608),\n",
      "   ('years', 0.0073762631850704434)])]\n",
      "\n",
      "Coherence Score:  0.4738965206360068\n"
     ]
    }
   ],
   "source": [
    "mallet_path = path.expanduser('~/work/jupyter_notebooks/notebooks/mallet-2.0.8/bin/mallet') # update this path\n",
    "prefix = path.expanduser('~/work/jupyter_notebooks/notebooks/mallet-dep/')\n",
    "model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=35, id2word=id2word, random_seed = 1, prefix=prefix)\n",
    "# Show Topics\n",
    "pprint(model.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Topics  10  - Coherence  0.3859702030700771\n",
      "No. Topics  20  - Coherence  0.45740016941974015\n",
      "No. Topics  30  - Coherence  0.45953520248675267\n",
      "No. Topics  40  - Coherence  0.461542256045851\n",
      "No. Topics  50  - Coherence  0.47737658463317856\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fe54da65a4e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Can take a long time to run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_coherence_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# Show graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fe54da65a4e6>\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[0;34m(dictionary, corpus, texts, limit, start, step)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmallet_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmodel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcoherencemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# NOTE \"--keep-sequence-bigrams\" / \"--use-ngrams true\" poorer results + runs out of memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training MALLET LDA with %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# NOTE - we are still keeping the wordtopics variable to not break backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"COMMAND: %s %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    924\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word, random_seed = 1)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        print('No. Topics ', num_topics,' - Coherence ',coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "limit=101; start=10; step=10;\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words, start=start, limit=limit, step=step)\n",
    "# Show graph\n",
    "\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 \n",
      " 0.030*\"energy\" + 0.028*\"water\" + 0.014*\"oil\" + 0.014*\"climate\" + 0.011*\"carbon\" + 0.011*\"power\" + 0.011*\"make\" + 0.011*\"percent\" + 0.010*\"fuel\" + 0.010*\"problem\" \n",
      "\n",
      "Topic 1 \n",
      " 0.040*\"sound\" + 0.039*\"play\" + 0.033*\"music\" + 0.021*\"hear\" + 0.015*\"song\" + 0.015*\"voice\" + 0.010*\"make\" + 0.010*\"call\" + 0.010*\"playing\" + 0.009*\"listen\" \n",
      "\n",
      "Topic 2 \n",
      " 0.029*\"company\" + 0.028*\"dollar\" + 0.028*\"money\" + 0.025*\"business\" + 0.015*\"pay\" + 0.015*\"market\" + 0.013*\"work\" + 0.012*\"make\" + 0.012*\"buy\" + 0.011*\"job\" \n",
      "\n",
      "Topic 3 \n",
      " 0.023*\"day\" + 0.022*\"back\" + 0.021*\"time\" + 0.014*\"start\" + 0.011*\"walk\" + 0.009*\"night\" + 0.009*\"put\" + 0.008*\"morning\" + 0.008*\"work\" + 0.008*\"hand\" \n",
      "\n",
      "Topic 4 \n",
      " 0.031*\"health\" + 0.019*\"disease\" + 0.016*\"care\" + 0.016*\"drug\" + 0.014*\"people\" + 0.013*\"doctor\" + 0.011*\"patient\" + 0.010*\"percent\" + 0.010*\"hospital\" + 0.010*\"medical\" \n",
      "\n",
      "Topic 5 \n",
      " 0.026*\"ocean\" + 0.016*\"fish\" + 0.016*\"sea\" + 0.015*\"water\" + 0.010*\"ice\" + 0.009*\"back\" + 0.008*\"coral\" + 0.007*\"place\" + 0.007*\"time\" + 0.007*\"shark\" \n",
      "\n",
      "Topic 6 \n",
      " 0.049*\"design\" + 0.033*\"make\" + 0.026*\"building\" + 0.017*\"material\" + 0.016*\"create\" + 0.016*\"build\" + 0.014*\"idea\" + 0.014*\"work\" + 0.012*\"kind\" + 0.012*\"project\" \n",
      "\n",
      "Topic 7 \n",
      " 0.044*\"love\" + 0.021*\"feel\" + 0.018*\"life\" + 0.015*\"fear\" + 0.011*\"hard\" + 0.010*\"make\" + 0.008*\"learn\" + 0.008*\"feeling\" + 0.008*\"felt\" + 0.007*\"friend\" \n",
      "\n",
      "Topic 8 \n",
      " 0.043*\"country\" + 0.039*\"world\" + 0.020*\"africa\" + 0.016*\"percent\" + 0.014*\"china\" + 0.014*\"state\" + 0.013*\"years\" + 0.011*\"global\" + 0.010*\"today\" + 0.010*\"growth\" \n",
      "\n",
      "Topic 9 \n",
      " 0.071*\"brain\" + 0.011*\"memory\" + 0.010*\"mind\" + 0.010*\"neuron\" + 0.009*\"control\" + 0.008*\"body\" + 0.007*\"change\" + 0.007*\"consciousness\" + 0.007*\"show\" + 0.006*\"pattern\" \n",
      "\n",
      "Topic 10 \n",
      " 0.055*\"things\" + 0.045*\"thing\" + 0.036*\"kind\" + 0.026*\"lot\" + 0.022*\"make\" + 0.022*\"sort\" + 0.021*\"start\" + 0.019*\"talk\" + 0.019*\"idea\" + 0.018*\"bit\" \n",
      "\n",
      "Topic 11 \n",
      " 0.047*\"car\" + 0.037*\"game\" + 0.020*\"ca\" + 0.015*\"world\" + 0.015*\"time\" + 0.012*\"play\" + 0.011*\"make\" + 0.011*\"drive\" + 0.009*\"years\" + 0.008*\"good\" \n",
      "\n",
      "Topic 12 \n",
      " 0.018*\"people\" + 0.016*\"political\" + 0.015*\"power\" + 0.011*\"american\" + 0.011*\"democracy\" + 0.009*\"religion\" + 0.008*\"politics\" + 0.008*\"vote\" + 0.007*\"state\" + 0.007*\"country\" \n",
      "\n",
      "Topic 13 \n",
      " 0.100*\"woman\" + 0.037*\"men\" + 0.029*\"girl\" + 0.016*\"black\" + 0.015*\"sex\" + 0.014*\"man\" + 0.011*\"young\" + 0.011*\"boy\" + 0.009*\"white\" + 0.009*\"talk\" \n",
      "\n",
      "Topic 14 \n",
      " 0.031*\"war\" + 0.012*\"people\" + 0.010*\"conflict\" + 0.010*\"kill\" + 0.009*\"military\" + 0.008*\"refugee\" + 0.008*\"world\" + 0.008*\"peace\" + 0.007*\"violence\" + 0.007*\"force\" \n",
      "\n",
      "Topic 15 \n",
      " 0.036*\"earth\" + 0.032*\"planet\" + 0.020*\"years\" + 0.016*\"life\" + 0.015*\"space\" + 0.011*\"mars\" + 0.011*\"system\" + 0.011*\"sun\" + 0.010*\"time\" + 0.008*\"place\" \n",
      "\n",
      "Topic 16 \n",
      " 0.021*\"data\" + 0.013*\"study\" + 0.011*\"question\" + 0.011*\"percent\" + 0.010*\"good\" + 0.010*\"model\" + 0.009*\"make\" + 0.009*\"answer\" + 0.009*\"result\" + 0.009*\"time\" \n",
      "\n",
      "Topic 17 \n",
      " 0.018*\"gene\" + 0.017*\"dna\" + 0.016*\"life\" + 0.015*\"cell\" + 0.014*\"make\" + 0.011*\"years\" + 0.009*\"molecule\" + 0.009*\"biology\" + 0.009*\"bacteria\" + 0.009*\"organism\" \n",
      "\n",
      "Topic 18 \n",
      " 0.061*\"school\" + 0.053*\"kid\" + 0.034*\"student\" + 0.021*\"education\" + 0.021*\"teacher\" + 0.020*\"learn\" + 0.020*\"child\" + 0.016*\"teach\" + 0.014*\"learning\" + 0.013*\"class\" \n",
      "\n",
      "Topic 19 \n",
      " 0.043*\"book\" + 0.031*\"write\" + 0.027*\"language\" + 0.021*\"read\" + 0.020*\"word\" + 0.019*\"words\" + 0.017*\"story\" + 0.012*\"english\" + 0.012*\"writing\" + 0.009*\"call\" \n",
      "\n",
      "Topic 20 \n",
      " 0.028*\"robot\" + 0.013*\"fly\" + 0.013*\"move\" + 0.012*\"body\" + 0.012*\"foot\" + 0.009*\"control\" + 0.008*\"legs\" + 0.007*\"make\" + 0.007*\"air\" + 0.007*\"run\" \n",
      "\n",
      "Topic 21 \n",
      " 0.034*\"cancer\" + 0.033*\"cell\" + 0.022*\"patient\" + 0.019*\"body\" + 0.017*\"blood\" + 0.014*\"disease\" + 0.013*\"heart\" + 0.009*\"surgery\" + 0.008*\"tumor\" + 0.007*\"doctor\" \n",
      "\n",
      "Topic 22 \n",
      " 0.171*\"people\" + 0.024*\"things\" + 0.022*\"change\" + 0.018*\"make\" + 0.018*\"good\" + 0.017*\"person\" + 0.016*\"lot\" + 0.015*\"give\" + 0.015*\"life\" + 0.012*\"time\" \n",
      "\n",
      "Topic 23 \n",
      " 0.038*\"technology\" + 0.038*\"computer\" + 0.028*\"machine\" + 0.019*\"system\" + 0.012*\"human\" + 0.012*\"world\" + 0.010*\"build\" + 0.010*\"information\" + 0.009*\"data\" + 0.009*\"future\" \n",
      "\n",
      "Topic 24 \n",
      " 0.066*\"city\" + 0.023*\"place\" + 0.021*\"people\" + 0.017*\"building\" + 0.016*\"street\" + 0.014*\"space\" + 0.011*\"york\" + 0.010*\"build\" + 0.010*\"community\" + 0.010*\"map\" \n",
      "\n",
      "Topic 25 \n",
      " 0.035*\"life\" + 0.032*\"story\" + 0.028*\"world\" + 0.019*\"experience\" + 0.015*\"god\" + 0.011*\"feel\" + 0.011*\"moment\" + 0.010*\"live\" + 0.010*\"time\" + 0.010*\"mind\" \n",
      "\n",
      "Topic 26 \n",
      " 0.020*\"animal\" + 0.020*\"tree\" + 0.015*\"forest\" + 0.014*\"plant\" + 0.014*\"species\" + 0.010*\"world\" + 0.009*\"bird\" + 0.008*\"nature\" + 0.008*\"bee\" + 0.007*\"years\" \n",
      "\n",
      "Topic 27 \n",
      " 0.023*\"universe\" + 0.017*\"light\" + 0.012*\"theory\" + 0.012*\"space\" + 0.011*\"science\" + 0.009*\"number\" + 0.009*\"physics\" + 0.009*\"galaxy\" + 0.009*\"particle\" + 0.009*\"star\" \n",
      "\n",
      "Topic 28 \n",
      " 0.016*\"government\" + 0.014*\"law\" + 0.011*\"police\" + 0.011*\"case\" + 0.011*\"state\" + 0.010*\"prison\" + 0.010*\"people\" + 0.008*\"public\" + 0.008*\"crime\" + 0.007*\"system\" \n",
      "\n",
      "Topic 29 \n",
      " 0.023*\"art\" + 0.023*\"image\" + 0.018*\"film\" + 0.015*\"make\" + 0.014*\"work\" + 0.014*\"show\" + 0.013*\"artist\" + 0.012*\"movie\" + 0.011*\"picture\" + 0.010*\"create\" \n",
      "\n",
      "Topic 30 \n",
      " 0.022*\"internet\" + 0.017*\"phone\" + 0.016*\"data\" + 0.016*\"information\" + 0.015*\"medium\" + 0.014*\"people\" + 0.013*\"online\" + 0.011*\"google\" + 0.011*\"world\" + 0.010*\"network\" \n",
      "\n",
      "Topic 31 \n",
      " 0.036*\"people\" + 0.021*\"community\" + 0.019*\"world\" + 0.013*\"work\" + 0.011*\"create\" + 0.011*\"start\" + 0.011*\"change\" + 0.008*\"working\" + 0.008*\"idea\" + 0.008*\"local\" \n",
      "\n",
      "Topic 32 \n",
      " 0.051*\"child\" + 0.038*\"family\" + 0.037*\"life\" + 0.025*\"mother\" + 0.023*\"years\" + 0.017*\"father\" + 0.017*\"baby\" + 0.016*\"home\" + 0.014*\"parent\" + 0.013*\"time\" \n",
      "\n",
      "Topic 33 \n",
      " 0.021*\"human\" + 0.012*\"idea\" + 0.011*\"problem\" + 0.011*\"kind\" + 0.011*\"social\" + 0.010*\"question\" + 0.009*\"society\" + 0.009*\"rule\" + 0.008*\"fact\" + 0.008*\"choice\" \n",
      "\n",
      "Topic 34 \n",
      " 0.048*\"food\" + 0.018*\"eat\" + 0.014*\"make\" + 0.010*\"farmer\" + 0.010*\"grow\" + 0.009*\"good\" + 0.008*\"call\" + 0.007*\"feed\" + 0.007*\"farm\" + 0.007*\"eating\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(35):\n",
    "    print('Topic', i, '\\n' ,model.print_topic(i, topn=10),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-4d79f88c566c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Can take a long time to run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_coherence_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_words_bigrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Show graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b67890b478df>\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[0;34m(dictionary, corpus, texts, limit, start, step)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaMallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmallet_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mmodel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcoherencemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# NOTE \"--keep-sequence-bigrams\" / \"--use-ngrams true\" poorer results + runs out of memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training MALLET LDA with %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# NOTE - we are still keeping the wordtopics variable to not break backward compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"COMMAND: %s %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    924\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "limit=52; start=33; step=1;\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words_bigrams, start=start, limit=limit, step=step)\n",
    "# Show graph\n",
    "\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.expanduser('~/work/test/lda/ldamallet.pkl'), 'wb') as output:\n",
    "    model.save(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.expanduser('~/work/test/lda/lda.pkl'), 'wb') as output:\n",
    "    pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.2056</td>\n",
       "      <td>school, kid, student, education, teacher, lear...</td>\n",
       "      <td>good morning great blow away whole thing fact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1856</td>\n",
       "      <td>energy, water, oil, climate, carbon, power, ma...</td>\n",
       "      <td>thank much chris truly great honor opportunity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.1607</td>\n",
       "      <td>technology, computer, machine, system, human, ...</td>\n",
       "      <td>hello voice mail old friend call tech support ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.2459</td>\n",
       "      <td>city, place, people, building, street, space, ...</td>\n",
       "      <td>today happy hear sustainable development save ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5551</td>\n",
       "      <td>country, world, africa, percent, china, state,...</td>\n",
       "      <td>years ago take task teach global development s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>life, story, world, experience, god, feel, mom...</td>\n",
       "      <td>thank tell challenge excite excitement get cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>day, back, time, start, walk, night, put, morn...</td>\n",
       "      <td>september morning seventh birthday come downst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>design, make, building, material, create, buil...</td>\n",
       "      <td>going present three project rapid fire much ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>book, write, language, read, word, words, stor...</td>\n",
       "      <td>wonderful back love wonderful gathering must w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.2059</td>\n",
       "      <td>life, story, world, experience, god, feel, mom...</td>\n",
       "      <td>often ask surprise book say get write would ne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0            18.0              0.2056   \n",
       "1            1             0.0              0.1856   \n",
       "2            2            23.0              0.1607   \n",
       "3            3            24.0              0.2459   \n",
       "4            4             8.0              0.5551   \n",
       "5            5            25.0              0.1752   \n",
       "6            6             3.0              0.2372   \n",
       "7            7             6.0              0.3210   \n",
       "8            8            19.0              0.1220   \n",
       "9            9            25.0              0.2059   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  school, kid, student, education, teacher, lear...   \n",
       "1  energy, water, oil, climate, carbon, power, ma...   \n",
       "2  technology, computer, machine, system, human, ...   \n",
       "3  city, place, people, building, street, space, ...   \n",
       "4  country, world, africa, percent, china, state,...   \n",
       "5  life, story, world, experience, god, feel, mom...   \n",
       "6  day, back, time, start, walk, night, put, morn...   \n",
       "7  design, make, building, material, create, buil...   \n",
       "8  book, write, language, read, word, words, stor...   \n",
       "9  life, story, world, experience, god, feel, mom...   \n",
       "\n",
       "                                                Text  \n",
       "0  good morning great blow away whole thing fact ...  \n",
       "1  thank much chris truly great honor opportunity...  \n",
       "2  hello voice mail old friend call tech support ...  \n",
       "3  today happy hear sustainable development save ...  \n",
       "4  years ago take task teach global development s...  \n",
       "5  thank tell challenge excite excitement get cha...  \n",
       "6  september morning seventh birthday come downst...  \n",
       "7  going present three project rapid fire much ti...  \n",
       "8  wonderful back love wonderful gathering must w...  \n",
       "9  often ask surprise book say get write would ne...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel, corpus=corpus, texts=data1):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=data1['transcript'].values)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.expanduser('~/work/test/lda/docdominanttopic.pkl'), 'wb') as output:\n",
    "    pickle.dump(df_dominant_topic, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>energy, water, oil, climate, carbon, power, ma...</td>\n",
       "      <td>well big announcement make today really excite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>sound, play, music, hear, song, voice, make, c...</td>\n",
       "      <td>adam ockelford promise much talking lot derek ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4428</td>\n",
       "      <td>company, dollar, money, business, pay, market,...</td>\n",
       "      <td>want talk social innovation social happen trip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5899</td>\n",
       "      <td>day, back, time, start, walk, night, put, morn...</td>\n",
       "      <td>brain magic brain magic brain magic indicate a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5385</td>\n",
       "      <td>health, disease, care, drug, people, doctor, p...</td>\n",
       "      <td>one first patient see pediatrician sol beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5609</td>\n",
       "      <td>ocean, fish, sea, water, ice, back, coral, pla...</td>\n",
       "      <td>fascinate lifetime beauty form function giant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.4658</td>\n",
       "      <td>design, make, building, material, create, buil...</td>\n",
       "      <td>architect often ask origin form design kind fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4784</td>\n",
       "      <td>love, feel, life, fear, hard, make, learn, fee...</td>\n",
       "      <td>age three hundred seventy two think deep regre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.6245</td>\n",
       "      <td>country, world, africa, percent, china, state,...</td>\n",
       "      <td>nice tonight working history income wealth dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5664</td>\n",
       "      <td>brain, memory, mind, neuron, control, body, ch...</td>\n",
       "      <td>well chris point study human brain function st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.3222</td>\n",
       "      <td>things, thing, kind, lot, make, sort, start, t...</td>\n",
       "      <td>daffodil hudson hello yeah oh yeah yeah yeah y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5246</td>\n",
       "      <td>car, game, ca, world, time, play, make, drive,...</td>\n",
       "      <td>karl benz invent automobile later year take fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5681</td>\n",
       "      <td>people, political, power, american, democracy,...</td>\n",
       "      <td>week ago chance go saudi arabia first thing wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>woman, men, girl, black, sex, man, young, boy,...</td>\n",
       "      <td>going share paradigm shifting perspective issu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.5037</td>\n",
       "      <td>war, people, conflict, kill, military, refugee...</td>\n",
       "      <td>iran israel best friend intend change position...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.7048</td>\n",
       "      <td>earth, planet, years, life, space, mars, syste...</td>\n",
       "      <td>look star night amaze see beautiful amaze see ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.4903</td>\n",
       "      <td>data, study, question, percent, good, model, m...</td>\n",
       "      <td>like start performance saying percent everythi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.6229</td>\n",
       "      <td>gene, dna, life, cell, make, years, molecule, ...</td>\n",
       "      <td>spread word magnificence spider much learn spi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.4914</td>\n",
       "      <td>school, kid, student, education, teacher, lear...</td>\n",
       "      <td>getting college education year investment grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.4499</td>\n",
       "      <td>book, write, language, read, word, words, stor...</td>\n",
       "      <td>lexicographer make dictionary job lexicographe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.5901</td>\n",
       "      <td>robot, fly, move, body, foot, control, legs, m...</td>\n",
       "      <td>want imagine student lab want create biologica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.6468</td>\n",
       "      <td>cancer, cell, patient, body, blood, disease, h...</td>\n",
       "      <td>actually painting hang countway library harvar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>people, things, change, make, good, person, lo...</td>\n",
       "      <td>want talk today little bit predictable irratio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.5206</td>\n",
       "      <td>technology, computer, machine, system, human, ...</td>\n",
       "      <td>evolve tool tool evolve us ancestor create han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.5324</td>\n",
       "      <td>city, place, people, building, street, space, ...</td>\n",
       "      <td>talk walkable city walkable city well want bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5608</td>\n",
       "      <td>life, story, world, experience, god, feel, mom...</td>\n",
       "      <td>speaking compassion islamic point view perhaps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.5407</td>\n",
       "      <td>animal, tree, forest, plant, species, world, b...</td>\n",
       "      <td>study ant like think organization work particu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>universe, light, theory, space, science, numbe...</td>\n",
       "      <td>whoa dude check killer equation sweet actually...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.4933</td>\n",
       "      <td>government, law, police, case, state, prison, ...</td>\n",
       "      <td>fbi responsible terrorism plot unite state org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>art, image, film, make, work, show, artist, mo...</td>\n",
       "      <td>might wonder wearing sunglasses one answer tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.4637</td>\n",
       "      <td>internet, phone, data, information, medium, pe...</td>\n",
       "      <td>understand world live tell story remixing shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.3867</td>\n",
       "      <td>people, community, world, work, create, start,...</td>\n",
       "      <td>want introduce amaze woman name davinia davini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.4729</td>\n",
       "      <td>child, family, life, mother, years, father, ba...</td>\n",
       "      <td>dannielle hadley life pennsylvania means life ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>human, idea, problem, kind, social, question, ...</td>\n",
       "      <td>question religious please raise hand right thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.5270</td>\n",
       "      <td>food, eat, make, farmer, grow, good, call, fee...</td>\n",
       "      <td>wheat bread whole wheat bread make new techniq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Num  Topic_Perc_Contrib  \\\n",
       "0         0.0              0.5946   \n",
       "1         1.0              0.5598   \n",
       "2         2.0              0.4428   \n",
       "3         3.0              0.5899   \n",
       "4         4.0              0.5385   \n",
       "5         5.0              0.5609   \n",
       "6         6.0              0.4658   \n",
       "7         7.0              0.4784   \n",
       "8         8.0              0.6245   \n",
       "9         9.0              0.5664   \n",
       "10       10.0              0.3222   \n",
       "11       11.0              0.5246   \n",
       "12       12.0              0.5681   \n",
       "13       13.0              0.4800   \n",
       "14       14.0              0.5037   \n",
       "15       15.0              0.7048   \n",
       "16       16.0              0.4903   \n",
       "17       17.0              0.6229   \n",
       "18       18.0              0.4914   \n",
       "19       19.0              0.4499   \n",
       "20       20.0              0.5901   \n",
       "21       21.0              0.6468   \n",
       "22       22.0              0.4588   \n",
       "23       23.0              0.5206   \n",
       "24       24.0              0.5324   \n",
       "25       25.0              0.5608   \n",
       "26       26.0              0.5407   \n",
       "27       27.0              0.7042   \n",
       "28       28.0              0.4933   \n",
       "29       29.0              0.4330   \n",
       "30       30.0              0.4637   \n",
       "31       31.0              0.3867   \n",
       "32       32.0              0.4729   \n",
       "33       33.0              0.3481   \n",
       "34       34.0              0.5270   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   energy, water, oil, climate, carbon, power, ma...   \n",
       "1   sound, play, music, hear, song, voice, make, c...   \n",
       "2   company, dollar, money, business, pay, market,...   \n",
       "3   day, back, time, start, walk, night, put, morn...   \n",
       "4   health, disease, care, drug, people, doctor, p...   \n",
       "5   ocean, fish, sea, water, ice, back, coral, pla...   \n",
       "6   design, make, building, material, create, buil...   \n",
       "7   love, feel, life, fear, hard, make, learn, fee...   \n",
       "8   country, world, africa, percent, china, state,...   \n",
       "9   brain, memory, mind, neuron, control, body, ch...   \n",
       "10  things, thing, kind, lot, make, sort, start, t...   \n",
       "11  car, game, ca, world, time, play, make, drive,...   \n",
       "12  people, political, power, american, democracy,...   \n",
       "13  woman, men, girl, black, sex, man, young, boy,...   \n",
       "14  war, people, conflict, kill, military, refugee...   \n",
       "15  earth, planet, years, life, space, mars, syste...   \n",
       "16  data, study, question, percent, good, model, m...   \n",
       "17  gene, dna, life, cell, make, years, molecule, ...   \n",
       "18  school, kid, student, education, teacher, lear...   \n",
       "19  book, write, language, read, word, words, stor...   \n",
       "20  robot, fly, move, body, foot, control, legs, m...   \n",
       "21  cancer, cell, patient, body, blood, disease, h...   \n",
       "22  people, things, change, make, good, person, lo...   \n",
       "23  technology, computer, machine, system, human, ...   \n",
       "24  city, place, people, building, street, space, ...   \n",
       "25  life, story, world, experience, god, feel, mom...   \n",
       "26  animal, tree, forest, plant, species, world, b...   \n",
       "27  universe, light, theory, space, science, numbe...   \n",
       "28  government, law, police, case, state, prison, ...   \n",
       "29  art, image, film, make, work, show, artist, mo...   \n",
       "30  internet, phone, data, information, medium, pe...   \n",
       "31  people, community, world, work, create, start,...   \n",
       "32  child, family, life, mother, years, father, ba...   \n",
       "33  human, idea, problem, kind, social, question, ...   \n",
       "34  food, eat, make, farmer, grow, good, call, fee...   \n",
       "\n",
       "                                                 Text  \n",
       "0   well big announcement make today really excite...  \n",
       "1   adam ockelford promise much talking lot derek ...  \n",
       "2   want talk social innovation social happen trip...  \n",
       "3   brain magic brain magic brain magic indicate a...  \n",
       "4   one first patient see pediatrician sol beautif...  \n",
       "5   fascinate lifetime beauty form function giant ...  \n",
       "6   architect often ask origin form design kind fo...  \n",
       "7   age three hundred seventy two think deep regre...  \n",
       "8   nice tonight working history income wealth dis...  \n",
       "9   well chris point study human brain function st...  \n",
       "10  daffodil hudson hello yeah oh yeah yeah yeah y...  \n",
       "11  karl benz invent automobile later year take fi...  \n",
       "12  week ago chance go saudi arabia first thing wa...  \n",
       "13  going share paradigm shifting perspective issu...  \n",
       "14  iran israel best friend intend change position...  \n",
       "15  look star night amaze see beautiful amaze see ...  \n",
       "16  like start performance saying percent everythi...  \n",
       "17  spread word magnificence spider much learn spi...  \n",
       "18  getting college education year investment grow...  \n",
       "19  lexicographer make dictionary job lexicographe...  \n",
       "20  want imagine student lab want create biologica...  \n",
       "21  actually painting hang countway library harvar...  \n",
       "22  want talk today little bit predictable irratio...  \n",
       "23  evolve tool tool evolve us ancestor create han...  \n",
       "24  talk walkable city walkable city well want bet...  \n",
       "25  speaking compassion islamic point view perhaps...  \n",
       "26  study ant like think organization work particu...  \n",
       "27  whoa dude check killer equation sweet actually...  \n",
       "28  fbi responsible terrorism plot unite state org...  \n",
       "29  might wonder wearing sunglasses one answer tal...  \n",
       "30  understand world live tell story remixing shar...  \n",
       "31  want introduce amaze woman name davinia davini...  \n",
       "32  dannielle hadley life pennsylvania means life ...  \n",
       "33  question religious please raise hand right thi...  \n",
       "34  wheat bread whole wheat bread make new techniq...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.expanduser('~/work/test/lda/topicdominantdoc.pkl'), 'wb') as output:\n",
    "    pickle.dump(sent_topics_sorteddf_mallet, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>school, kid, student, education, teacher, lear...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>energy, water, oil, climate, carbon, power, ma...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>technology, computer, machine, system, human, ...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>city, place, people, building, street, space, ...</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>country, world, africa, percent, china, state,...</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.0354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2452.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>child, family, life, mother, years, father, ba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2453.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>earth, planet, years, life, space, mars, syste...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2454.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>human, idea, problem, kind, social, question, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2455.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>people, political, power, american, democracy,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2456.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>city, place, people, building, street, space, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2457 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic                                     Topic_Keywords  \\\n",
       "0.0               18.0  school, kid, student, education, teacher, lear...   \n",
       "1.0                0.0  energy, water, oil, climate, carbon, power, ma...   \n",
       "2.0               23.0  technology, computer, machine, system, human, ...   \n",
       "3.0               24.0  city, place, people, building, street, space, ...   \n",
       "4.0                8.0  country, world, africa, percent, china, state,...   \n",
       "...                ...                                                ...   \n",
       "2452.0            32.0  child, family, life, mother, years, father, ba...   \n",
       "2453.0            15.0  earth, planet, years, life, space, mars, syste...   \n",
       "2454.0            33.0  human, idea, problem, kind, social, question, ...   \n",
       "2455.0            12.0  people, political, power, american, democracy,...   \n",
       "2456.0            24.0  city, place, people, building, street, space, ...   \n",
       "\n",
       "        Num_Documents  Perc_Documents  \n",
       "0.0              82.0          0.0334  \n",
       "1.0              90.0          0.0366  \n",
       "2.0              75.0          0.0305  \n",
       "3.0              88.0          0.0358  \n",
       "4.0              87.0          0.0354  \n",
       "...               ...             ...  \n",
       "2452.0            NaN             NaN  \n",
       "2453.0            NaN             NaN  \n",
       "2454.0            NaN             NaN  \n",
       "2455.0            NaN             NaN  \n",
       "2456.0            NaN             NaN  \n",
       "\n",
       "[2457 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.expanduser('~/work/test/lda/topicdist.pkl'), 'wb') as output:\n",
    "    pickle.dump(df_dominant_topics, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet.to_csv(path.expanduser('~/work/test/lda/topics.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/jupyter_notebooks/notebooks/mallet-dep/'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
